{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7228bd61",
   "metadata": {},
   "source": [
    "## Credit Card Payer Default Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c07bd9",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Calculate probability of default for each payer based on payment history and couple of demographic variables. Probabality of default converted to a threshold by use of a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf30c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import pyrsm as rsm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from statsmodels.genmod.families import Binomial\n",
    "from statsmodels.genmod.families.links import logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5abaa5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\">\n",
       "  <input type=\"submit\" id=\"toggleButton\" value=\"Show Python 3 Code\">\n",
       "</form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "  <input type=\"submit\" id=\"toggleButton\" value=\"Show Python 3 Code\">\n",
    "</form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6d8bf",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Courtesy UCI Mahine Learning Repository\n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3c1df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d70a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            int64\n",
       "LIMIT_BAL     int64\n",
       "GENDER       object\n",
       "EDUC         object\n",
       "MARITAL      object\n",
       "AGE           int64\n",
       "PAY_1         int64\n",
       "PAY_2         int64\n",
       "PAY_3         int64\n",
       "PAY_4         int64\n",
       "PAY_5         int64\n",
       "PAY_6         int64\n",
       "BILL_AMT1     int64\n",
       "BILL_AMT2     int64\n",
       "BILL_AMT3     int64\n",
       "BILL_AMT4     int64\n",
       "BILL_AMT5     int64\n",
       "BILL_AMT6     int64\n",
       "PAY_AMT1      int64\n",
       "PAY_AMT2      int64\n",
       "PAY_AMT3      int64\n",
       "PAY_AMT4      int64\n",
       "PAY_AMT5      int64\n",
       "PAY_AMT6      int64\n",
       "DEFAULT       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"credit_card.csv - credit_card.csv.csv\")\n",
    "\n",
    "# df = df[df['BILL_AMT4'] != -170000]\n",
    "# df = df[df['BILL_AMT4'] != -81334]\n",
    "\n",
    "# df = df[df['BILL_AMT1'] >= 0]\n",
    "# df = df[df['BILL_AMT2'] >= 0]\n",
    "# df = df[df['BILL_AMT3'] >= 0]\n",
    "# df = df[df['BILL_AMT4'] >= 0]\n",
    "# df = df[df['BILL_AMT5'] >= 0]\n",
    "# df = df[df['BILL_AMT6'] >= 0]\n",
    "\n",
    "# df = df.reset_index(drop = True)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5956cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.barplot(x = df['MARITAL'].astype('category'), y = df['DEFAULT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfb07b",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "Data exploration suggested converting Limit_balance to bins and making a categorical variable along with demographic variables (gender, education, marital status) set as categorical variables as well. <br>\n",
    "\n",
    "Feature engineering was done to create new variables: <br>\n",
    "a.) spend proportion: what was the proportion of the bill amt for the months April-Sep to the limit balance? <br>\n",
    "b.) On a monthly rolled up basis, accumulating from April to Sep, what proportion of the bill amount was paid? <br>\n",
    "c.) On a monthly rolled up basis, accumulating from April to Sep, what is the outstanding balance to limit balance proportion? <br>\n",
    "\n",
    "Categorical variables were one-hot encoded and numerical variables were standardized. <br>\n",
    "4 models: logistic, random forest, xg boosted tree, and neural network performance was compared. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408001a3",
   "metadata": {},
   "source": [
    "### Value to stakeholder\n",
    "\n",
    "The models achieved on a detection rate of ~31% (i.e. of all the actual defaulters, how many did the model catch?) and 81% accuracy. <br>\n",
    "Profit calculations from value of detecting a defaulter and cost of falsely flagging a customer defaulter can be weighed for economic impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=list(range(0, 550000, 25000)) + [1000000]\n",
    "labels=list(range(25000, 550000, 25000)) + [1000000]\n",
    "df['LIMIT_BAL_BINS'] = pd.cut(df['LIMIT_BAL'], bins=bins,labels=labels).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0e9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,0, -1):\n",
    "    \n",
    "    df['SPEND_PROP_' + str(i)] = df['BILL_AMT' + str(i)] / df['LIMIT_BAL']   # ratio of each month's spend to the limit\n",
    "    \n",
    "    \n",
    "temp1 = 0\n",
    "temp2 = 0\n",
    "\n",
    "temp3 = 0\n",
    "temp4 = 0\n",
    "    \n",
    "for i in range(5,0, -1):\n",
    "    \n",
    "    temp1+= df['PAY_AMT' + str(i)]\n",
    "    temp2+= df['BILL_AMT' + str(i+1)]\n",
    "    \n",
    "    \n",
    "    df['ROLLING_PAY_PROP_' + str(i)] = np.where(temp2 == 0, 1, temp1 / temp2) \n",
    "    # rolling sum of payment / rolling sum of bill amt\n",
    "    #temp2 == 0 implies no bill amt generated, nothing due for the user to pay  \n",
    "        \n",
    "    df['ROLLING_OUTBAL_TO_LIMIT_' + str(i)] = np.array(temp2 - temp1) / df['LIMIT_BAL']   # rolling sum of outstanding balance (i.e. bill - payment) / limit_balance\n",
    "        \n",
    "    \n",
    "#     temp3 = df['PAY_AMT' + str(i)]\n",
    "#     temp4 = df['BILL_AMT' + str(i+1)]\n",
    "    \n",
    "    \n",
    "#     df['MONTHLY_PAY_PROP_' + str(i)] = np.where(temp4 == 0, 1, temp3 / temp4 )\n",
    "#     # temp3 == 0, implies no bill amt generated, nothing due for the user to pay\n",
    "#     # montly payment / monthly bill amt\n",
    "        \n",
    "    \n",
    "#     df['MONTHLY_OUTBAL_TO_LIMIT_' + str(i)] = np.array(temp4 - temp3) / df['LIMIT_BAL']   # monthly outstanding balance (i.e. bill - payment) / limit_balance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a7676bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "catg = ['GENDER', 'EDUC', 'MARITAL', 'LIMIT_BAL_BINS']\n",
    "\n",
    "num = list(df.loc[:, 'AGE':'PAY_AMT6'].columns) + list(df.loc[:, 'SPEND_PROP_6':].columns)\n",
    "\n",
    "df[catg] = df[catg].astype('category')\n",
    "\n",
    "df['GENDER'] = df['GENDER'].cat.reorder_categories(['M', 'F'])\n",
    "df['EDUC'] = df['EDUC'].cat.reorder_categories(['HS', 'Bachelors', 'Masters', 'Other'])\n",
    "df['MARITAL'] = df['MARITAL'].cat.reorder_categories(['Married', 'Single', 'Other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fd3665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGE',\n",
       " 'PAY_1',\n",
       " 'PAY_2',\n",
       " 'PAY_3',\n",
       " 'PAY_4',\n",
       " 'PAY_5',\n",
       " 'PAY_6',\n",
       " 'BILL_AMT1',\n",
       " 'BILL_AMT2',\n",
       " 'BILL_AMT3',\n",
       " 'BILL_AMT4',\n",
       " 'BILL_AMT5',\n",
       " 'BILL_AMT6',\n",
       " 'PAY_AMT1',\n",
       " 'PAY_AMT2',\n",
       " 'PAY_AMT3',\n",
       " 'PAY_AMT4',\n",
       " 'PAY_AMT5',\n",
       " 'PAY_AMT6',\n",
       " 'SPEND_PROP_6',\n",
       " 'SPEND_PROP_5',\n",
       " 'SPEND_PROP_4',\n",
       " 'SPEND_PROP_3',\n",
       " 'SPEND_PROP_2',\n",
       " 'SPEND_PROP_1',\n",
       " 'ROLLING_PAY_PROP_5',\n",
       " 'ROLLING_OUTBAL_TO_LIMIT_5',\n",
       " 'ROLLING_PAY_PROP_4',\n",
       " 'ROLLING_OUTBAL_TO_LIMIT_4',\n",
       " 'ROLLING_PAY_PROP_3',\n",
       " 'ROLLING_OUTBAL_TO_LIMIT_3',\n",
       " 'ROLLING_PAY_PROP_2',\n",
       " 'ROLLING_OUTBAL_TO_LIMIT_2',\n",
       " 'ROLLING_PAY_PROP_1',\n",
       " 'ROLLING_OUTBAL_TO_LIMIT_1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bca4dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GENDER', 'EDUC', 'MARITAL', 'LIMIT_BAL_BINS']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f985341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "\n",
    "sssplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "for train_index, test_index in sssplit.split(df['DEFAULT'], df['DEFAULT']):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7200dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['training'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c20f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[train_index, 'training'] = 1\n",
    "df.loc[test_index, 'training'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77d3d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER_F</th>\n",
       "      <th>EDUC_Bachelors</th>\n",
       "      <th>EDUC_Masters</th>\n",
       "      <th>EDUC_Other</th>\n",
       "      <th>MARITAL_Single</th>\n",
       "      <th>MARITAL_Other</th>\n",
       "      <th>LIMIT_BAL_BINS_50000</th>\n",
       "      <th>LIMIT_BAL_BINS_75000</th>\n",
       "      <th>LIMIT_BAL_BINS_100000</th>\n",
       "      <th>LIMIT_BAL_BINS_125000</th>\n",
       "      <th>...</th>\n",
       "      <th>ROLLING_PAY_PROP_5</th>\n",
       "      <th>ROLLING_OUTBAL_TO_LIMIT_5</th>\n",
       "      <th>ROLLING_PAY_PROP_4</th>\n",
       "      <th>ROLLING_OUTBAL_TO_LIMIT_4</th>\n",
       "      <th>ROLLING_PAY_PROP_3</th>\n",
       "      <th>ROLLING_OUTBAL_TO_LIMIT_3</th>\n",
       "      <th>ROLLING_PAY_PROP_2</th>\n",
       "      <th>ROLLING_OUTBAL_TO_LIMIT_2</th>\n",
       "      <th>ROLLING_PAY_PROP_1</th>\n",
       "      <th>ROLLING_OUTBAL_TO_LIMIT_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052695</td>\n",
       "      <td>-0.423805</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.440543</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>-0.455713</td>\n",
       "      <td>0.027899</td>\n",
       "      <td>-0.469612</td>\n",
       "      <td>-0.016951</td>\n",
       "      <td>-0.435466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046326</td>\n",
       "      <td>-0.383153</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>-0.404343</td>\n",
       "      <td>-0.027417</td>\n",
       "      <td>-0.422012</td>\n",
       "      <td>-0.014865</td>\n",
       "      <td>-0.439264</td>\n",
       "      <td>-0.014756</td>\n",
       "      <td>-0.453437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039957</td>\n",
       "      <td>-0.181978</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>-0.199912</td>\n",
       "      <td>-0.043561</td>\n",
       "      <td>-0.220420</td>\n",
       "      <td>-0.023814</td>\n",
       "      <td>-0.244121</td>\n",
       "      <td>-0.025074</td>\n",
       "      <td>-0.261371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042743</td>\n",
       "      <td>0.428220</td>\n",
       "      <td>-0.010662</td>\n",
       "      <td>0.415744</td>\n",
       "      <td>-0.046966</td>\n",
       "      <td>0.389267</td>\n",
       "      <td>-0.025915</td>\n",
       "      <td>0.515010</td>\n",
       "      <td>-0.028618</td>\n",
       "      <td>0.575703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042759</td>\n",
       "      <td>0.127956</td>\n",
       "      <td>-0.003278</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>-0.011401</td>\n",
       "      <td>-0.055474</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>-0.178254</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>-0.228710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015343</td>\n",
       "      <td>-0.349144</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.305234</td>\n",
       "      <td>-0.039978</td>\n",
       "      <td>-0.174568</td>\n",
       "      <td>-0.022743</td>\n",
       "      <td>0.061978</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.190303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052695</td>\n",
       "      <td>-0.423805</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.414902</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>-0.438696</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.457013</td>\n",
       "      <td>0.029277</td>\n",
       "      <td>-0.471833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036095</td>\n",
       "      <td>0.441694</td>\n",
       "      <td>-0.006623</td>\n",
       "      <td>0.414143</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.094724</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>-0.025531</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>-0.096041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060828</td>\n",
       "      <td>-0.498975</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>-0.384410</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>-0.091798</td>\n",
       "      <td>-0.010581</td>\n",
       "      <td>0.144256</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>-0.022847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039859</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>-0.010497</td>\n",
       "      <td>0.254691</td>\n",
       "      <td>-0.046737</td>\n",
       "      <td>0.362890</td>\n",
       "      <td>-0.025946</td>\n",
       "      <td>0.500601</td>\n",
       "      <td>-0.028628</td>\n",
       "      <td>0.567821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GENDER_F  EDUC_Bachelors  EDUC_Masters  EDUC_Other  MARITAL_Single  \\\n",
       "0             1               1             0           0               0   \n",
       "1             1               1             0           0               1   \n",
       "2             1               1             0           0               1   \n",
       "3             1               1             0           0               0   \n",
       "4             0               1             0           0               0   \n",
       "...         ...             ...           ...         ...             ...   \n",
       "29995         0               0             0           0               0   \n",
       "29996         0               0             0           0               1   \n",
       "29997         0               1             0           0               1   \n",
       "29998         0               0             0           0               0   \n",
       "29999         0               1             0           0               0   \n",
       "\n",
       "       MARITAL_Other  LIMIT_BAL_BINS_50000  LIMIT_BAL_BINS_75000  \\\n",
       "0                  0                     0                     0   \n",
       "1                  0                     0                     0   \n",
       "2                  0                     0                     0   \n",
       "3                  0                     1                     0   \n",
       "4                  0                     1                     0   \n",
       "...              ...                   ...                   ...   \n",
       "29995              0                     0                     0   \n",
       "29996              0                     0                     0   \n",
       "29997              0                     1                     0   \n",
       "29998              0                     0                     0   \n",
       "29999              0                     1                     0   \n",
       "\n",
       "       LIMIT_BAL_BINS_100000  LIMIT_BAL_BINS_125000  ...  ROLLING_PAY_PROP_5  \\\n",
       "0                          0                      0  ...            0.052695   \n",
       "1                          0                      1  ...           -0.046326   \n",
       "2                          1                      0  ...           -0.039957   \n",
       "3                          0                      0  ...           -0.042743   \n",
       "4                          0                      0  ...           -0.042759   \n",
       "...                      ...                    ...  ...                 ...   \n",
       "29995                      0                      0  ...           -0.015343   \n",
       "29996                      0                      0  ...            0.052695   \n",
       "29997                      0                      0  ...           -0.036095   \n",
       "29998                      1                      0  ...            0.060828   \n",
       "29999                      0                      0  ...           -0.039859   \n",
       "\n",
       "       ROLLING_OUTBAL_TO_LIMIT_5  ROLLING_PAY_PROP_4  \\\n",
       "0                      -0.423805            0.022250   \n",
       "1                      -0.383153           -0.006840   \n",
       "2                      -0.181978           -0.009688   \n",
       "3                       0.428220           -0.010662   \n",
       "4                       0.127956           -0.003278   \n",
       "...                          ...                 ...   \n",
       "29995                  -0.349144           -0.006104   \n",
       "29996                  -0.423805           -0.011080   \n",
       "29997                   0.441694           -0.006623   \n",
       "29998                  -0.498975            0.018928   \n",
       "29999                   0.004422           -0.010497   \n",
       "\n",
       "       ROLLING_OUTBAL_TO_LIMIT_4  ROLLING_PAY_PROP_3  \\\n",
       "0                      -0.440543            0.069434   \n",
       "1                      -0.404343           -0.027417   \n",
       "2                      -0.199912           -0.043561   \n",
       "3                       0.415744           -0.046966   \n",
       "4                      -0.006024           -0.011401   \n",
       "...                          ...                 ...   \n",
       "29995                  -0.305234           -0.039978   \n",
       "29996                  -0.414902            0.026341   \n",
       "29997                   0.414143            0.004486   \n",
       "29998                  -0.384410            0.008118   \n",
       "29999                   0.254691           -0.046737   \n",
       "\n",
       "       ROLLING_OUTBAL_TO_LIMIT_3  ROLLING_PAY_PROP_2  \\\n",
       "0                      -0.455713            0.027899   \n",
       "1                      -0.422012           -0.014865   \n",
       "2                      -0.220420           -0.023814   \n",
       "3                       0.389267           -0.025915   \n",
       "4                      -0.055474            0.005097   \n",
       "...                          ...                 ...   \n",
       "29995                  -0.174568           -0.022743   \n",
       "29996                  -0.438696            0.011988   \n",
       "29997                   0.094724           -0.003279   \n",
       "29998                  -0.091798           -0.010581   \n",
       "29999                   0.362890           -0.025946   \n",
       "\n",
       "       ROLLING_OUTBAL_TO_LIMIT_2  ROLLING_PAY_PROP_1  \\\n",
       "0                      -0.469612           -0.016951   \n",
       "1                      -0.439264           -0.014756   \n",
       "2                      -0.244121           -0.025074   \n",
       "3                       0.515010           -0.028618   \n",
       "4                      -0.178254            0.015804   \n",
       "...                          ...                 ...   \n",
       "29995                   0.061978           -0.025539   \n",
       "29996                  -0.457013            0.029277   \n",
       "29997                  -0.025531            0.002778   \n",
       "29998                   0.144256            0.012710   \n",
       "29999                   0.500601           -0.028628   \n",
       "\n",
       "       ROLLING_OUTBAL_TO_LIMIT_1  \n",
       "0                      -0.435466  \n",
       "1                      -0.453437  \n",
       "2                      -0.261371  \n",
       "3                       0.575703  \n",
       "4                      -0.228710  \n",
       "...                          ...  \n",
       "29995                   0.190303  \n",
       "29996                  -0.471833  \n",
       "29997                  -0.096041  \n",
       "29998                  -0.022847  \n",
       "29999                   0.567821  \n",
       "\n",
       "[30000 rows x 62 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = pd.concat([pd.get_dummies(df[catg], drop_first = True), rsm.scale_df(df[num], train=df['training'] == 1)], axis = 1)\n",
    "\n",
    "Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5b26a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d19fb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEFAULT ~ GENDER + EDUC + MARITAL + LIMIT_BAL_BINS + AGE + PAY_1 + PAY_2 + PAY_3 + PAY_4 + PAY_5 + PAY_6 + BILL_AMT1 + BILL_AMT2 + BILL_AMT3 + BILL_AMT4 + BILL_AMT5 + BILL_AMT6 + PAY_AMT1 + PAY_AMT2 + PAY_AMT3 + PAY_AMT4 + PAY_AMT5 + PAY_AMT6 + SPEND_PROP_6 + SPEND_PROP_5 + SPEND_PROP_4 + SPEND_PROP_3 + SPEND_PROP_2 + SPEND_PROP_1 + ROLLING_PAY_PROP_5 + ROLLING_OUTBAL_TO_LIMIT_5 + ROLLING_PAY_PROP_4 + ROLLING_OUTBAL_TO_LIMIT_4 + ROLLING_PAY_PROP_3 + ROLLING_OUTBAL_TO_LIMIT_3 + ROLLING_PAY_PROP_2 + ROLLING_OUTBAL_TO_LIMIT_2 + ROLLING_PAY_PROP_1 + ROLLING_OUTBAL_TO_LIMIT_1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model = \"DEFAULT ~ \" + ' + '.join(catg + num)\n",
    "logit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f656678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>DEFAULT</td>     <th>  No. Observations:  </th>  <td> 24000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 23937</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>    62</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -11073.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 04 Apr 2022</td> <th>  Deviance:          </th> <td>  22147.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:08:15</td>     <th>  Pearson chi2:      </th> <td>2.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>HC1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>   -0.5473</td> <td>    0.130</td> <td>   -4.210</td> <td> 0.000</td> <td>   -0.802</td> <td>   -0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.F]</th>               <td>   -0.0671</td> <td>    0.035</td> <td>   -1.941</td> <td> 0.052</td> <td>   -0.135</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUC[T.Bachelors]</th>         <td>    0.0167</td> <td>    0.048</td> <td>    0.351</td> <td> 0.726</td> <td>   -0.077</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUC[T.Masters]</th>           <td>    0.0632</td> <td>    0.054</td> <td>    1.181</td> <td> 0.238</td> <td>   -0.042</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUC[T.Other]</th>             <td>   -1.1090</td> <td>    0.227</td> <td>   -4.889</td> <td> 0.000</td> <td>   -1.554</td> <td>   -0.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARITAL[T.Single]</th>         <td>   -0.2055</td> <td>    0.039</td> <td>   -5.317</td> <td> 0.000</td> <td>   -0.281</td> <td>   -0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARITAL[T.Other]</th>          <td>   -0.2302</td> <td>    0.151</td> <td>   -1.520</td> <td> 0.128</td> <td>   -0.527</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.50000]</th>   <td>   -0.3087</td> <td>    0.068</td> <td>   -4.540</td> <td> 0.000</td> <td>   -0.442</td> <td>   -0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.75000]</th>   <td>   -0.3131</td> <td>    0.090</td> <td>   -3.483</td> <td> 0.000</td> <td>   -0.489</td> <td>   -0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.100000]</th>  <td>   -0.5037</td> <td>    0.082</td> <td>   -6.139</td> <td> 0.000</td> <td>   -0.665</td> <td>   -0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.125000]</th>  <td>   -0.5362</td> <td>    0.104</td> <td>   -5.152</td> <td> 0.000</td> <td>   -0.740</td> <td>   -0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.150000]</th>  <td>   -0.6284</td> <td>    0.096</td> <td>   -6.525</td> <td> 0.000</td> <td>   -0.817</td> <td>   -0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.175000]</th>  <td>   -0.7761</td> <td>    0.121</td> <td>   -6.395</td> <td> 0.000</td> <td>   -1.014</td> <td>   -0.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.200000]</th>  <td>   -0.6767</td> <td>    0.103</td> <td>   -6.567</td> <td> 0.000</td> <td>   -0.879</td> <td>   -0.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.225000]</th>  <td>   -0.7593</td> <td>    0.127</td> <td>   -5.995</td> <td> 0.000</td> <td>   -1.008</td> <td>   -0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.250000]</th>  <td>   -0.7764</td> <td>    0.119</td> <td>   -6.509</td> <td> 0.000</td> <td>   -1.010</td> <td>   -0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.275000]</th>  <td>   -0.7834</td> <td>    0.149</td> <td>   -5.251</td> <td> 0.000</td> <td>   -1.076</td> <td>   -0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.300000]</th>  <td>   -0.7516</td> <td>    0.130</td> <td>   -5.767</td> <td> 0.000</td> <td>   -1.007</td> <td>   -0.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.325000]</th>  <td>   -1.1848</td> <td>    0.186</td> <td>   -6.381</td> <td> 0.000</td> <td>   -1.549</td> <td>   -0.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.350000]</th>  <td>   -0.8751</td> <td>    0.173</td> <td>   -5.067</td> <td> 0.000</td> <td>   -1.214</td> <td>   -0.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.375000]</th>  <td>   -0.5485</td> <td>    0.142</td> <td>   -3.858</td> <td> 0.000</td> <td>   -0.827</td> <td>   -0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.400000]</th>  <td>   -1.0692</td> <td>    0.186</td> <td>   -5.741</td> <td> 0.000</td> <td>   -1.434</td> <td>   -0.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.425000]</th>  <td>   -1.0352</td> <td>    0.260</td> <td>   -3.983</td> <td> 0.000</td> <td>   -1.545</td> <td>   -0.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.450000]</th>  <td>   -0.7807</td> <td>    0.214</td> <td>   -3.656</td> <td> 0.000</td> <td>   -1.199</td> <td>   -0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.475000]</th>  <td>   -1.1220</td> <td>    0.292</td> <td>   -3.848</td> <td> 0.000</td> <td>   -1.693</td> <td>   -0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.500000]</th>  <td>   -1.0712</td> <td>    0.181</td> <td>   -5.928</td> <td> 0.000</td> <td>   -1.425</td> <td>   -0.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.525000]</th>  <td>   -1.3638</td> <td>    0.586</td> <td>   -2.326</td> <td> 0.020</td> <td>   -2.513</td> <td>   -0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL_BINS[T.1000000]</th> <td>   -1.2271</td> <td>    0.327</td> <td>   -3.751</td> <td> 0.000</td> <td>   -1.868</td> <td>   -0.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>                       <td>    0.0048</td> <td>    0.002</td> <td>    2.287</td> <td> 0.022</td> <td>    0.001</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_1</th>                     <td>    0.5698</td> <td>    0.023</td> <td>   24.965</td> <td> 0.000</td> <td>    0.525</td> <td>    0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>                     <td>    0.1068</td> <td>    0.025</td> <td>    4.273</td> <td> 0.000</td> <td>    0.058</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>                     <td>    0.0483</td> <td>    0.028</td> <td>    1.721</td> <td> 0.085</td> <td>   -0.007</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>                     <td>    0.0277</td> <td>    0.031</td> <td>    0.902</td> <td> 0.367</td> <td>   -0.032</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>                     <td>    0.0478</td> <td>    0.032</td> <td>    1.480</td> <td> 0.139</td> <td>   -0.016</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>                     <td>   -0.0179</td> <td>    0.027</td> <td>   -0.669</td> <td> 0.504</td> <td>   -0.070</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th>                 <td>-1.198e-06</td> <td> 2.27e-06</td> <td>   -0.527</td> <td> 0.598</td> <td>-5.65e-06</td> <td> 3.26e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th>                 <td>  3.71e-06</td> <td> 3.01e-06</td> <td>    1.234</td> <td> 0.217</td> <td>-2.18e-06</td> <td>  9.6e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th>                 <td>-9.657e-07</td> <td>  2.3e-06</td> <td>   -0.419</td> <td> 0.675</td> <td>-5.48e-06</td> <td> 3.55e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th>                 <td>-1.772e-06</td> <td> 2.13e-06</td> <td>   -0.832</td> <td> 0.405</td> <td>-5.95e-06</td> <td>  2.4e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th>                 <td>  2.92e-06</td> <td> 2.36e-06</td> <td>    1.238</td> <td> 0.216</td> <td> -1.7e-06</td> <td> 7.54e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th>                 <td>-1.906e-06</td> <td> 1.83e-06</td> <td>   -1.040</td> <td> 0.299</td> <td> -5.5e-06</td> <td> 1.69e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>                  <td> -1.58e-05</td> <td> 5.58e-06</td> <td>   -2.830</td> <td> 0.005</td> <td>-2.67e-05</td> <td>-4.86e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>                  <td> 2.974e-07</td> <td> 4.59e-06</td> <td>    0.065</td> <td> 0.948</td> <td> -8.7e-06</td> <td> 9.29e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>                  <td>-3.466e-06</td> <td> 3.65e-06</td> <td>   -0.949</td> <td> 0.342</td> <td>-1.06e-05</td> <td> 3.69e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>                  <td> -3.34e-06</td> <td> 2.79e-06</td> <td>   -1.196</td> <td> 0.232</td> <td>-8.81e-06</td> <td> 2.13e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>                  <td> 4.376e-07</td> <td> 3.09e-06</td> <td>    0.142</td> <td> 0.887</td> <td>-5.61e-06</td> <td> 6.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>                  <td>-2.914e-06</td> <td> 1.85e-06</td> <td>   -1.573</td> <td> 0.116</td> <td>-6.55e-06</td> <td> 7.18e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_6</th>              <td>   -0.2029</td> <td>    0.323</td> <td>   -0.629</td> <td> 0.529</td> <td>   -0.835</td> <td>    0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_5</th>              <td>   -0.5013</td> <td>    0.331</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.149</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_4</th>              <td>    0.1731</td> <td>    0.366</td> <td>    0.473</td> <td> 0.636</td> <td>   -0.545</td> <td>    0.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_3</th>              <td>   -1.2160</td> <td>    0.471</td> <td>   -2.580</td> <td> 0.010</td> <td>   -2.140</td> <td>   -0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_2</th>              <td>    0.0177</td> <td>    0.395</td> <td>    0.045</td> <td> 0.964</td> <td>   -0.757</td> <td>    0.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPEND_PROP_1</th>              <td>   -0.6991</td> <td>    0.255</td> <td>   -2.745</td> <td> 0.006</td> <td>   -1.198</td> <td>   -0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_PAY_PROP_5</th>        <td>    0.0058</td> <td>    0.002</td> <td>    2.563</td> <td> 0.010</td> <td>    0.001</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_OUTBAL_TO_LIMIT_5</th> <td>    0.6744</td> <td>    0.501</td> <td>    1.346</td> <td> 0.178</td> <td>   -0.307</td> <td>    1.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_PAY_PROP_4</th>        <td>    0.0025</td> <td>    0.003</td> <td>    0.870</td> <td> 0.384</td> <td>   -0.003</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_OUTBAL_TO_LIMIT_4</th> <td>   -0.1352</td> <td>    0.497</td> <td>   -0.272</td> <td> 0.786</td> <td>   -1.110</td> <td>    0.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_PAY_PROP_3</th>        <td>    0.0040</td> <td>    0.005</td> <td>    0.755</td> <td> 0.450</td> <td>   -0.006</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_OUTBAL_TO_LIMIT_3</th> <td>   -1.4778</td> <td>    0.660</td> <td>   -2.238</td> <td> 0.025</td> <td>   -2.772</td> <td>   -0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_PAY_PROP_2</th>        <td>   -0.0026</td> <td>    0.001</td> <td>   -2.025</td> <td> 0.043</td> <td>   -0.005</td> <td>-8.23e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_OUTBAL_TO_LIMIT_2</th> <td>    1.8689</td> <td>    0.730</td> <td>    2.560</td> <td> 0.010</td> <td>    0.438</td> <td>    3.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_PAY_PROP_1</th>        <td>    0.0075</td> <td>    0.007</td> <td>    1.148</td> <td> 0.251</td> <td>   -0.005</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ROLLING_OUTBAL_TO_LIMIT_1</th> <td>   -0.2717</td> <td>    0.438</td> <td>   -0.620</td> <td> 0.535</td> <td>   -1.131</td> <td>    0.588</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                DEFAULT   No. Observations:                24000\n",
       "Model:                            GLM   Df Residuals:                    23937\n",
       "Model Family:                Binomial   Df Model:                           62\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -11073.\n",
       "Date:                Mon, 04 Apr 2022   Deviance:                       22147.\n",
       "Time:                        13:08:15   Pearson chi2:                 2.91e+04\n",
       "No. Iterations:                     6                                         \n",
       "Covariance Type:                  HC1                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                    -0.5473      0.130     -4.210      0.000      -0.802      -0.293\n",
       "GENDER[T.F]                  -0.0671      0.035     -1.941      0.052      -0.135       0.001\n",
       "EDUC[T.Bachelors]             0.0167      0.048      0.351      0.726      -0.077       0.110\n",
       "EDUC[T.Masters]               0.0632      0.054      1.181      0.238      -0.042       0.168\n",
       "EDUC[T.Other]                -1.1090      0.227     -4.889      0.000      -1.554      -0.664\n",
       "MARITAL[T.Single]            -0.2055      0.039     -5.317      0.000      -0.281      -0.130\n",
       "MARITAL[T.Other]             -0.2302      0.151     -1.520      0.128      -0.527       0.067\n",
       "LIMIT_BAL_BINS[T.50000]      -0.3087      0.068     -4.540      0.000      -0.442      -0.175\n",
       "LIMIT_BAL_BINS[T.75000]      -0.3131      0.090     -3.483      0.000      -0.489      -0.137\n",
       "LIMIT_BAL_BINS[T.100000]     -0.5037      0.082     -6.139      0.000      -0.665      -0.343\n",
       "LIMIT_BAL_BINS[T.125000]     -0.5362      0.104     -5.152      0.000      -0.740      -0.332\n",
       "LIMIT_BAL_BINS[T.150000]     -0.6284      0.096     -6.525      0.000      -0.817      -0.440\n",
       "LIMIT_BAL_BINS[T.175000]     -0.7761      0.121     -6.395      0.000      -1.014      -0.538\n",
       "LIMIT_BAL_BINS[T.200000]     -0.6767      0.103     -6.567      0.000      -0.879      -0.475\n",
       "LIMIT_BAL_BINS[T.225000]     -0.7593      0.127     -5.995      0.000      -1.008      -0.511\n",
       "LIMIT_BAL_BINS[T.250000]     -0.7764      0.119     -6.509      0.000      -1.010      -0.543\n",
       "LIMIT_BAL_BINS[T.275000]     -0.7834      0.149     -5.251      0.000      -1.076      -0.491\n",
       "LIMIT_BAL_BINS[T.300000]     -0.7516      0.130     -5.767      0.000      -1.007      -0.496\n",
       "LIMIT_BAL_BINS[T.325000]     -1.1848      0.186     -6.381      0.000      -1.549      -0.821\n",
       "LIMIT_BAL_BINS[T.350000]     -0.8751      0.173     -5.067      0.000      -1.214      -0.537\n",
       "LIMIT_BAL_BINS[T.375000]     -0.5485      0.142     -3.858      0.000      -0.827      -0.270\n",
       "LIMIT_BAL_BINS[T.400000]     -1.0692      0.186     -5.741      0.000      -1.434      -0.704\n",
       "LIMIT_BAL_BINS[T.425000]     -1.0352      0.260     -3.983      0.000      -1.545      -0.526\n",
       "LIMIT_BAL_BINS[T.450000]     -0.7807      0.214     -3.656      0.000      -1.199      -0.362\n",
       "LIMIT_BAL_BINS[T.475000]     -1.1220      0.292     -3.848      0.000      -1.693      -0.551\n",
       "LIMIT_BAL_BINS[T.500000]     -1.0712      0.181     -5.928      0.000      -1.425      -0.717\n",
       "LIMIT_BAL_BINS[T.525000]     -1.3638      0.586     -2.326      0.020      -2.513      -0.214\n",
       "LIMIT_BAL_BINS[T.1000000]    -1.2271      0.327     -3.751      0.000      -1.868      -0.586\n",
       "AGE                           0.0048      0.002      2.287      0.022       0.001       0.009\n",
       "PAY_1                         0.5698      0.023     24.965      0.000       0.525       0.615\n",
       "PAY_2                         0.1068      0.025      4.273      0.000       0.058       0.156\n",
       "PAY_3                         0.0483      0.028      1.721      0.085      -0.007       0.103\n",
       "PAY_4                         0.0277      0.031      0.902      0.367      -0.032       0.088\n",
       "PAY_5                         0.0478      0.032      1.480      0.139      -0.016       0.111\n",
       "PAY_6                        -0.0179      0.027     -0.669      0.504      -0.070       0.034\n",
       "BILL_AMT1                 -1.198e-06   2.27e-06     -0.527      0.598   -5.65e-06    3.26e-06\n",
       "BILL_AMT2                   3.71e-06   3.01e-06      1.234      0.217   -2.18e-06     9.6e-06\n",
       "BILL_AMT3                 -9.657e-07    2.3e-06     -0.419      0.675   -5.48e-06    3.55e-06\n",
       "BILL_AMT4                 -1.772e-06   2.13e-06     -0.832      0.405   -5.95e-06     2.4e-06\n",
       "BILL_AMT5                   2.92e-06   2.36e-06      1.238      0.216    -1.7e-06    7.54e-06\n",
       "BILL_AMT6                 -1.906e-06   1.83e-06     -1.040      0.299    -5.5e-06    1.69e-06\n",
       "PAY_AMT1                   -1.58e-05   5.58e-06     -2.830      0.005   -2.67e-05   -4.86e-06\n",
       "PAY_AMT2                   2.974e-07   4.59e-06      0.065      0.948    -8.7e-06    9.29e-06\n",
       "PAY_AMT3                  -3.466e-06   3.65e-06     -0.949      0.342   -1.06e-05    3.69e-06\n",
       "PAY_AMT4                   -3.34e-06   2.79e-06     -1.196      0.232   -8.81e-06    2.13e-06\n",
       "PAY_AMT5                   4.376e-07   3.09e-06      0.142      0.887   -5.61e-06    6.48e-06\n",
       "PAY_AMT6                  -2.914e-06   1.85e-06     -1.573      0.116   -6.55e-06    7.18e-07\n",
       "SPEND_PROP_6                 -0.2029      0.323     -0.629      0.529      -0.835       0.429\n",
       "SPEND_PROP_5                 -0.5013      0.331     -1.516      0.130      -1.149       0.147\n",
       "SPEND_PROP_4                  0.1731      0.366      0.473      0.636      -0.545       0.891\n",
       "SPEND_PROP_3                 -1.2160      0.471     -2.580      0.010      -2.140      -0.292\n",
       "SPEND_PROP_2                  0.0177      0.395      0.045      0.964      -0.757       0.793\n",
       "SPEND_PROP_1                 -0.6991      0.255     -2.745      0.006      -1.198      -0.200\n",
       "ROLLING_PAY_PROP_5            0.0058      0.002      2.563      0.010       0.001       0.010\n",
       "ROLLING_OUTBAL_TO_LIMIT_5     0.6744      0.501      1.346      0.178      -0.307       1.656\n",
       "ROLLING_PAY_PROP_4            0.0025      0.003      0.870      0.384      -0.003       0.008\n",
       "ROLLING_OUTBAL_TO_LIMIT_4    -0.1352      0.497     -0.272      0.786      -1.110       0.839\n",
       "ROLLING_PAY_PROP_3            0.0040      0.005      0.755      0.450      -0.006       0.015\n",
       "ROLLING_OUTBAL_TO_LIMIT_3    -1.4778      0.660     -2.238      0.025      -2.772      -0.184\n",
       "ROLLING_PAY_PROP_2           -0.0026      0.001     -2.025      0.043      -0.005   -8.23e-05\n",
       "ROLLING_OUTBAL_TO_LIMIT_2     1.8689      0.730      2.560      0.010       0.438       3.300\n",
       "ROLLING_PAY_PROP_1            0.0075      0.007      1.148      0.251      -0.005       0.020\n",
       "ROLLING_OUTBAL_TO_LIMIT_1    -0.2717      0.438     -0.620      0.535      -1.131       0.588\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = smf.glm(\n",
    " formula=logit_model,\n",
    " family=Binomial(link=logit()),\n",
    " data=df[df.training == 1]\n",
    " ).fit(cov_type=\"HC1\")\n",
    "\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14f0f1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>OR</th>\n",
       "      <th>OR%</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>p.values</th>\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_2</td>\n",
       "      <td>6.481</td>\n",
       "      <td>548.1%</td>\n",
       "      <td>1.549</td>\n",
       "      <td>27.108</td>\n",
       "      <td>0.01</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>6.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_3</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-77.2%</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.025</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>4.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LIMIT_BAL_BINS[T.525000]</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-74.4%</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.02</td>\n",
       "      <td>*</td>\n",
       "      <td>True</td>\n",
       "      <td>3.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LIMIT_BAL_BINS[T.1000000]</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-70.7%</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.557</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SPEND_PROP_3</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-70.4%</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.01</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>3.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LIMIT_BAL_BINS[T.325000]</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-69.4%</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.440</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LIMIT_BAL_BINS[T.475000]</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-67.4%</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.577</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDUC[T.Other]</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-67.0%</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.515</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LIMIT_BAL_BINS[T.500000]</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-65.7%</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.488</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LIMIT_BAL_BINS[T.400000]</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-65.7%</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.495</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LIMIT_BAL_BINS[T.425000]</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-64.5%</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.591</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LIMIT_BAL_BINS[T.350000]</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-58.3%</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.585</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LIMIT_BAL_BINS[T.275000]</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-54.3%</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.612</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LIMIT_BAL_BINS[T.450000]</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-54.2%</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.696</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LIMIT_BAL_BINS[T.250000]</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-54.0%</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.581</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LIMIT_BAL_BINS[T.175000]</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-54.0%</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.584</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LIMIT_BAL_BINS[T.225000]</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-53.2%</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.600</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LIMIT_BAL_BINS[T.300000]</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-52.8%</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.609</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SPEND_PROP_1</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-50.3%</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.006</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>2.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LIMIT_BAL_BINS[T.200000]</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-49.2%</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.622</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_5</td>\n",
       "      <td>1.963</td>\n",
       "      <td>96.3%</td>\n",
       "      <td>0.735</td>\n",
       "      <td>5.239</td>\n",
       "      <td>0.178</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LIMIT_BAL_BINS[T.150000]</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-46.7%</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.644</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PAY_1</td>\n",
       "      <td>1.768</td>\n",
       "      <td>76.8%</td>\n",
       "      <td>1.691</td>\n",
       "      <td>1.849</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>False</td>\n",
       "      <td>1.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LIMIT_BAL_BINS[T.375000]</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-42.2%</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.763</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LIMIT_BAL_BINS[T.125000]</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-41.5%</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.717</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LIMIT_BAL_BINS[T.100000]</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-39.6%</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.710</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>SPEND_PROP_5</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-39.4%</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.158</td>\n",
       "      <td>0.13</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LIMIT_BAL_BINS[T.75000]</td>\n",
       "      <td>0.731</td>\n",
       "      <td>-26.9%</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.872</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LIMIT_BAL_BINS[T.50000]</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-26.6%</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.839</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_1</td>\n",
       "      <td>0.762</td>\n",
       "      <td>-23.8%</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0.535</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARITAL[T.Other]</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-20.6%</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.128</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MARITAL[T.Single]</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-18.6%</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.878</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SPEND_PROP_6</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-18.4%</td>\n",
       "      <td>0.434</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.529</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SPEND_PROP_4</td>\n",
       "      <td>1.189</td>\n",
       "      <td>18.9%</td>\n",
       "      <td>0.580</td>\n",
       "      <td>2.437</td>\n",
       "      <td>0.636</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_4</td>\n",
       "      <td>0.874</td>\n",
       "      <td>-12.6%</td>\n",
       "      <td>0.330</td>\n",
       "      <td>2.315</td>\n",
       "      <td>0.786</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>1.113</td>\n",
       "      <td>11.3%</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.169</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>False</td>\n",
       "      <td>1.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENDER[T.F]</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-6.5%</td>\n",
       "      <td>0.874</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.052</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>1.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDUC[T.Masters]</td>\n",
       "      <td>1.065</td>\n",
       "      <td>6.5%</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.238</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>1.049</td>\n",
       "      <td>4.9%</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.085</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>1.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>1.049</td>\n",
       "      <td>4.9%</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.118</td>\n",
       "      <td>0.139</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>1.028</td>\n",
       "      <td>2.8%</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.367</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SPEND_PROP_2</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.8%</td>\n",
       "      <td>0.469</td>\n",
       "      <td>2.210</td>\n",
       "      <td>0.964</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-1.8%</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.504</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDUC[T.Bachelors]</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.7%</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.726</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>ROLLING_PAY_PROP_1</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.251</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ROLLING_PAY_PROP_5</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.6%</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AGE</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ROLLING_PAY_PROP_3</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.4%</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.45</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ROLLING_PAY_PROP_2</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.3%</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ROLLING_PAY_PROP_4</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.2%</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.384</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.299</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.887</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.948</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.598</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        index     OR     OR%   2.5%   97.5% p.values       \\\n",
       "60  ROLLING_OUTBAL_TO_LIMIT_2  6.481  548.1%  1.549  27.108     0.01    *   \n",
       "58  ROLLING_OUTBAL_TO_LIMIT_3  0.228  -77.2%  0.063   0.832    0.025    *   \n",
       "26   LIMIT_BAL_BINS[T.525000]  0.256  -74.4%  0.081   0.807     0.02    *   \n",
       "27  LIMIT_BAL_BINS[T.1000000]  0.293  -70.7%  0.154   0.557   < .001  ***   \n",
       "50               SPEND_PROP_3  0.296  -70.4%  0.118   0.747     0.01   **   \n",
       "18   LIMIT_BAL_BINS[T.325000]  0.306  -69.4%  0.213   0.440   < .001  ***   \n",
       "24   LIMIT_BAL_BINS[T.475000]  0.326  -67.4%  0.184   0.577   < .001  ***   \n",
       "4               EDUC[T.Other]  0.330  -67.0%  0.211   0.515   < .001  ***   \n",
       "25   LIMIT_BAL_BINS[T.500000]  0.343  -65.7%  0.240   0.488   < .001  ***   \n",
       "21   LIMIT_BAL_BINS[T.400000]  0.343  -65.7%  0.238   0.495   < .001  ***   \n",
       "22   LIMIT_BAL_BINS[T.425000]  0.355  -64.5%  0.213   0.591   < .001  ***   \n",
       "19   LIMIT_BAL_BINS[T.350000]  0.417  -58.3%  0.297   0.585   < .001  ***   \n",
       "16   LIMIT_BAL_BINS[T.275000]  0.457  -54.3%  0.341   0.612   < .001  ***   \n",
       "23   LIMIT_BAL_BINS[T.450000]  0.458  -54.2%  0.301   0.696   < .001  ***   \n",
       "15   LIMIT_BAL_BINS[T.250000]  0.460  -54.0%  0.364   0.581   < .001  ***   \n",
       "12   LIMIT_BAL_BINS[T.175000]  0.460  -54.0%  0.363   0.584   < .001  ***   \n",
       "14   LIMIT_BAL_BINS[T.225000]  0.468  -53.2%  0.365   0.600   < .001  ***   \n",
       "17   LIMIT_BAL_BINS[T.300000]  0.472  -52.8%  0.365   0.609   < .001  ***   \n",
       "52               SPEND_PROP_1  0.497  -50.3%  0.302   0.819    0.006   **   \n",
       "13   LIMIT_BAL_BINS[T.200000]  0.508  -49.2%  0.415   0.622   < .001  ***   \n",
       "54  ROLLING_OUTBAL_TO_LIMIT_5  1.963   96.3%  0.735   5.239    0.178        \n",
       "11   LIMIT_BAL_BINS[T.150000]  0.533  -46.7%  0.442   0.644   < .001  ***   \n",
       "29                      PAY_1  1.768   76.8%  1.691   1.849   < .001  ***   \n",
       "20   LIMIT_BAL_BINS[T.375000]  0.578  -42.2%  0.437   0.763   < .001  ***   \n",
       "10   LIMIT_BAL_BINS[T.125000]  0.585  -41.5%  0.477   0.717   < .001  ***   \n",
       "9    LIMIT_BAL_BINS[T.100000]  0.604  -39.6%  0.515   0.710   < .001  ***   \n",
       "48               SPEND_PROP_5  0.606  -39.4%  0.317   1.158     0.13        \n",
       "8     LIMIT_BAL_BINS[T.75000]  0.731  -26.9%  0.613   0.872   < .001  ***   \n",
       "7     LIMIT_BAL_BINS[T.50000]  0.734  -26.6%  0.643   0.839   < .001  ***   \n",
       "62  ROLLING_OUTBAL_TO_LIMIT_1  0.762  -23.8%  0.323   1.800    0.535        \n",
       "6            MARITAL[T.Other]  0.794  -20.6%  0.590   1.069    0.128        \n",
       "5           MARITAL[T.Single]  0.814  -18.6%  0.755   0.878   < .001  ***   \n",
       "47               SPEND_PROP_6  0.816  -18.4%  0.434   1.536    0.529        \n",
       "49               SPEND_PROP_4  1.189   18.9%  0.580   2.437    0.636        \n",
       "56  ROLLING_OUTBAL_TO_LIMIT_4  0.874  -12.6%  0.330   2.315    0.786        \n",
       "30                      PAY_2  1.113   11.3%  1.060   1.169   < .001  ***   \n",
       "1                 GENDER[T.F]  0.935   -6.5%  0.874   1.001    0.052    .   \n",
       "3             EDUC[T.Masters]  1.065    6.5%  0.959   1.183    0.238        \n",
       "31                      PAY_3  1.049    4.9%  0.993   1.109    0.085    .   \n",
       "33                      PAY_5  1.049    4.9%  0.985   1.118    0.139        \n",
       "32                      PAY_4  1.028    2.8%  0.968   1.092    0.367        \n",
       "51               SPEND_PROP_2  1.018    1.8%  0.469   2.210    0.964        \n",
       "34                      PAY_6  0.982   -1.8%  0.932   1.035    0.504        \n",
       "2           EDUC[T.Bachelors]  1.017    1.7%  0.926   1.116    0.726        \n",
       "61         ROLLING_PAY_PROP_1  1.008    0.8%  0.995   1.021    0.251        \n",
       "53         ROLLING_PAY_PROP_5  1.006    0.6%  1.001   1.010     0.01    *   \n",
       "28                        AGE  1.005    0.5%  1.001   1.009    0.022    *   \n",
       "57         ROLLING_PAY_PROP_3  1.004    0.4%  0.994   1.015     0.45        \n",
       "59         ROLLING_PAY_PROP_2  0.997   -0.3%  0.995   1.000    0.043    *   \n",
       "55         ROLLING_PAY_PROP_4  1.002    0.2%  0.997   1.008    0.384        \n",
       "40                  BILL_AMT6  1.000   -0.0%  1.000   1.000    0.299        \n",
       "41                   PAY_AMT1  1.000   -0.0%  1.000   1.000    0.005   **   \n",
       "44                   PAY_AMT4  1.000   -0.0%  1.000   1.000    0.232        \n",
       "45                   PAY_AMT5  1.000    0.0%  1.000   1.000    0.887        \n",
       "42                   PAY_AMT2  1.000    0.0%  1.000   1.000    0.948        \n",
       "43                   PAY_AMT3  1.000   -0.0%  1.000   1.000    0.342        \n",
       "39                  BILL_AMT5  1.000    0.0%  1.000   1.000    0.216        \n",
       "35                  BILL_AMT1  1.000   -0.0%  1.000   1.000    0.598        \n",
       "36                  BILL_AMT2  1.000    0.0%  1.000   1.000    0.217        \n",
       "37                  BILL_AMT3  1.000   -0.0%  1.000   1.000    0.675        \n",
       "\n",
       "    dummy  importance  \n",
       "60  False       6.481  \n",
       "58  False       4.383  \n",
       "26   True       3.911  \n",
       "27   True       3.411  \n",
       "50  False       3.374  \n",
       "18   True       3.270  \n",
       "24   True       3.071  \n",
       "4    True       3.031  \n",
       "25   True       2.919  \n",
       "21   True       2.913  \n",
       "22   True       2.816  \n",
       "19   True       2.399  \n",
       "16   True       2.189  \n",
       "23   True       2.183  \n",
       "15   True       2.174  \n",
       "12   True       2.173  \n",
       "14   True       2.137  \n",
       "17   True       2.120  \n",
       "52  False       2.012  \n",
       "13   True       1.967  \n",
       "54  False       1.963  \n",
       "11   True       1.875  \n",
       "29  False       1.768  \n",
       "20   True       1.731  \n",
       "10   True       1.709  \n",
       "9    True       1.655  \n",
       "48  False       1.651  \n",
       "8    True       1.368  \n",
       "7    True       1.362  \n",
       "62  False       1.312  \n",
       "6    True       1.259  \n",
       "5    True       1.228  \n",
       "47  False       1.225  \n",
       "49  False       1.189  \n",
       "56  False       1.145  \n",
       "30  False       1.113  \n",
       "1    True       1.069  \n",
       "3    True       1.065  \n",
       "31  False       1.049  \n",
       "33  False       1.049  \n",
       "32  False       1.028  \n",
       "51  False       1.018  \n",
       "34  False       1.018  \n",
       "2    True       1.017  \n",
       "61  False       1.008  \n",
       "53  False       1.006  \n",
       "28  False       1.005  \n",
       "57  False       1.004  \n",
       "59  False       1.003  \n",
       "55  False       1.002  \n",
       "40  False       1.000  \n",
       "41  False       1.000  \n",
       "44  False       1.000  \n",
       "45  False       1.000  \n",
       "42  False       1.000  \n",
       "43  False       1.000  \n",
       "39  False       1.000  \n",
       "35  False       1.000  \n",
       "36  False       1.000  \n",
       "37  False       1.000  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.or_ci(lr, importance=True).sort_values(\"importance\", ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ee00f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>OR</th>\n",
       "      <th>OR%</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>p.values</th>\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENDER[T.F]</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-6.5%</td>\n",
       "      <td>0.874</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.052</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>1.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EDUC[T.Bachelors]</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.7%</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.726</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDUC[T.Masters]</td>\n",
       "      <td>1.065</td>\n",
       "      <td>6.5%</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.238</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDUC[T.Other]</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-67.0%</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.515</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MARITAL[T.Single]</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-18.6%</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.878</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARITAL[T.Other]</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-20.6%</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.128</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LIMIT_BAL_BINS[T.50000]</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-26.6%</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.839</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LIMIT_BAL_BINS[T.75000]</td>\n",
       "      <td>0.731</td>\n",
       "      <td>-26.9%</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.872</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LIMIT_BAL_BINS[T.100000]</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-39.6%</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.710</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LIMIT_BAL_BINS[T.125000]</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-41.5%</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.717</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LIMIT_BAL_BINS[T.150000]</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-46.7%</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.644</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LIMIT_BAL_BINS[T.175000]</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-54.0%</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.584</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LIMIT_BAL_BINS[T.200000]</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-49.2%</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.622</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LIMIT_BAL_BINS[T.225000]</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-53.2%</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.600</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LIMIT_BAL_BINS[T.250000]</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-54.0%</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.581</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LIMIT_BAL_BINS[T.275000]</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-54.3%</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.612</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LIMIT_BAL_BINS[T.300000]</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-52.8%</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.609</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LIMIT_BAL_BINS[T.325000]</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-69.4%</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.440</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LIMIT_BAL_BINS[T.350000]</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-58.3%</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.585</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LIMIT_BAL_BINS[T.375000]</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-42.2%</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.763</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>1.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LIMIT_BAL_BINS[T.400000]</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-65.7%</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.495</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LIMIT_BAL_BINS[T.425000]</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-64.5%</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.591</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LIMIT_BAL_BINS[T.450000]</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-54.2%</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.696</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LIMIT_BAL_BINS[T.475000]</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-67.4%</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.577</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LIMIT_BAL_BINS[T.500000]</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-65.7%</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.488</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>2.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LIMIT_BAL_BINS[T.525000]</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-74.4%</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.02</td>\n",
       "      <td>*</td>\n",
       "      <td>True</td>\n",
       "      <td>3.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LIMIT_BAL_BINS[T.1000000]</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-70.7%</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.557</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>True</td>\n",
       "      <td>3.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AGE</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PAY_1</td>\n",
       "      <td>1.768</td>\n",
       "      <td>76.8%</td>\n",
       "      <td>1.691</td>\n",
       "      <td>1.849</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>False</td>\n",
       "      <td>1.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>1.113</td>\n",
       "      <td>11.3%</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.169</td>\n",
       "      <td>&lt; .001</td>\n",
       "      <td>***</td>\n",
       "      <td>False</td>\n",
       "      <td>1.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>1.049</td>\n",
       "      <td>4.9%</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.085</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>1.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>1.028</td>\n",
       "      <td>2.8%</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.367</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>1.049</td>\n",
       "      <td>4.9%</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.118</td>\n",
       "      <td>0.139</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-1.8%</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.504</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.598</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.299</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.948</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.887</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.0%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SPEND_PROP_6</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-18.4%</td>\n",
       "      <td>0.434</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.529</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>SPEND_PROP_5</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-39.4%</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.158</td>\n",
       "      <td>0.13</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SPEND_PROP_4</td>\n",
       "      <td>1.189</td>\n",
       "      <td>18.9%</td>\n",
       "      <td>0.580</td>\n",
       "      <td>2.437</td>\n",
       "      <td>0.636</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SPEND_PROP_3</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-70.4%</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.01</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>3.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SPEND_PROP_2</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.8%</td>\n",
       "      <td>0.469</td>\n",
       "      <td>2.210</td>\n",
       "      <td>0.964</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SPEND_PROP_1</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-50.3%</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.006</td>\n",
       "      <td>**</td>\n",
       "      <td>False</td>\n",
       "      <td>2.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ROLLING_PAY_PROP_5</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.6%</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_5</td>\n",
       "      <td>1.963</td>\n",
       "      <td>96.3%</td>\n",
       "      <td>0.735</td>\n",
       "      <td>5.239</td>\n",
       "      <td>0.178</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ROLLING_PAY_PROP_4</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.2%</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.384</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_4</td>\n",
       "      <td>0.874</td>\n",
       "      <td>-12.6%</td>\n",
       "      <td>0.330</td>\n",
       "      <td>2.315</td>\n",
       "      <td>0.786</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ROLLING_PAY_PROP_3</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.4%</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.45</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_3</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-77.2%</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.025</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>4.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ROLLING_PAY_PROP_2</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.3%</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>1.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_2</td>\n",
       "      <td>6.481</td>\n",
       "      <td>548.1%</td>\n",
       "      <td>1.549</td>\n",
       "      <td>27.108</td>\n",
       "      <td>0.01</td>\n",
       "      <td>*</td>\n",
       "      <td>False</td>\n",
       "      <td>6.481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        index     OR     OR%   2.5%   97.5% p.values       \\\n",
       "1                 GENDER[T.F]  0.935   -6.5%  0.874   1.001    0.052    .   \n",
       "2           EDUC[T.Bachelors]  1.017    1.7%  0.926   1.116    0.726        \n",
       "3             EDUC[T.Masters]  1.065    6.5%  0.959   1.183    0.238        \n",
       "4               EDUC[T.Other]  0.330  -67.0%  0.211   0.515   < .001  ***   \n",
       "5           MARITAL[T.Single]  0.814  -18.6%  0.755   0.878   < .001  ***   \n",
       "6            MARITAL[T.Other]  0.794  -20.6%  0.590   1.069    0.128        \n",
       "7     LIMIT_BAL_BINS[T.50000]  0.734  -26.6%  0.643   0.839   < .001  ***   \n",
       "8     LIMIT_BAL_BINS[T.75000]  0.731  -26.9%  0.613   0.872   < .001  ***   \n",
       "9    LIMIT_BAL_BINS[T.100000]  0.604  -39.6%  0.515   0.710   < .001  ***   \n",
       "10   LIMIT_BAL_BINS[T.125000]  0.585  -41.5%  0.477   0.717   < .001  ***   \n",
       "11   LIMIT_BAL_BINS[T.150000]  0.533  -46.7%  0.442   0.644   < .001  ***   \n",
       "12   LIMIT_BAL_BINS[T.175000]  0.460  -54.0%  0.363   0.584   < .001  ***   \n",
       "13   LIMIT_BAL_BINS[T.200000]  0.508  -49.2%  0.415   0.622   < .001  ***   \n",
       "14   LIMIT_BAL_BINS[T.225000]  0.468  -53.2%  0.365   0.600   < .001  ***   \n",
       "15   LIMIT_BAL_BINS[T.250000]  0.460  -54.0%  0.364   0.581   < .001  ***   \n",
       "16   LIMIT_BAL_BINS[T.275000]  0.457  -54.3%  0.341   0.612   < .001  ***   \n",
       "17   LIMIT_BAL_BINS[T.300000]  0.472  -52.8%  0.365   0.609   < .001  ***   \n",
       "18   LIMIT_BAL_BINS[T.325000]  0.306  -69.4%  0.213   0.440   < .001  ***   \n",
       "19   LIMIT_BAL_BINS[T.350000]  0.417  -58.3%  0.297   0.585   < .001  ***   \n",
       "20   LIMIT_BAL_BINS[T.375000]  0.578  -42.2%  0.437   0.763   < .001  ***   \n",
       "21   LIMIT_BAL_BINS[T.400000]  0.343  -65.7%  0.238   0.495   < .001  ***   \n",
       "22   LIMIT_BAL_BINS[T.425000]  0.355  -64.5%  0.213   0.591   < .001  ***   \n",
       "23   LIMIT_BAL_BINS[T.450000]  0.458  -54.2%  0.301   0.696   < .001  ***   \n",
       "24   LIMIT_BAL_BINS[T.475000]  0.326  -67.4%  0.184   0.577   < .001  ***   \n",
       "25   LIMIT_BAL_BINS[T.500000]  0.343  -65.7%  0.240   0.488   < .001  ***   \n",
       "26   LIMIT_BAL_BINS[T.525000]  0.256  -74.4%  0.081   0.807     0.02    *   \n",
       "27  LIMIT_BAL_BINS[T.1000000]  0.293  -70.7%  0.154   0.557   < .001  ***   \n",
       "28                        AGE  1.005    0.5%  1.001   1.009    0.022    *   \n",
       "29                      PAY_1  1.768   76.8%  1.691   1.849   < .001  ***   \n",
       "30                      PAY_2  1.113   11.3%  1.060   1.169   < .001  ***   \n",
       "31                      PAY_3  1.049    4.9%  0.993   1.109    0.085    .   \n",
       "32                      PAY_4  1.028    2.8%  0.968   1.092    0.367        \n",
       "33                      PAY_5  1.049    4.9%  0.985   1.118    0.139        \n",
       "34                      PAY_6  0.982   -1.8%  0.932   1.035    0.504        \n",
       "35                  BILL_AMT1  1.000   -0.0%  1.000   1.000    0.598        \n",
       "36                  BILL_AMT2  1.000    0.0%  1.000   1.000    0.217        \n",
       "37                  BILL_AMT3  1.000   -0.0%  1.000   1.000    0.675        \n",
       "38                  BILL_AMT4  1.000   -0.0%  1.000   1.000    0.405        \n",
       "39                  BILL_AMT5  1.000    0.0%  1.000   1.000    0.216        \n",
       "40                  BILL_AMT6  1.000   -0.0%  1.000   1.000    0.299        \n",
       "41                   PAY_AMT1  1.000   -0.0%  1.000   1.000    0.005   **   \n",
       "42                   PAY_AMT2  1.000    0.0%  1.000   1.000    0.948        \n",
       "43                   PAY_AMT3  1.000   -0.0%  1.000   1.000    0.342        \n",
       "44                   PAY_AMT4  1.000   -0.0%  1.000   1.000    0.232        \n",
       "45                   PAY_AMT5  1.000    0.0%  1.000   1.000    0.887        \n",
       "46                   PAY_AMT6  1.000   -0.0%  1.000   1.000    0.116        \n",
       "47               SPEND_PROP_6  0.816  -18.4%  0.434   1.536    0.529        \n",
       "48               SPEND_PROP_5  0.606  -39.4%  0.317   1.158     0.13        \n",
       "49               SPEND_PROP_4  1.189   18.9%  0.580   2.437    0.636        \n",
       "50               SPEND_PROP_3  0.296  -70.4%  0.118   0.747     0.01   **   \n",
       "51               SPEND_PROP_2  1.018    1.8%  0.469   2.210    0.964        \n",
       "52               SPEND_PROP_1  0.497  -50.3%  0.302   0.819    0.006   **   \n",
       "53         ROLLING_PAY_PROP_5  1.006    0.6%  1.001   1.010     0.01    *   \n",
       "54  ROLLING_OUTBAL_TO_LIMIT_5  1.963   96.3%  0.735   5.239    0.178        \n",
       "55         ROLLING_PAY_PROP_4  1.002    0.2%  0.997   1.008    0.384        \n",
       "56  ROLLING_OUTBAL_TO_LIMIT_4  0.874  -12.6%  0.330   2.315    0.786        \n",
       "57         ROLLING_PAY_PROP_3  1.004    0.4%  0.994   1.015     0.45        \n",
       "58  ROLLING_OUTBAL_TO_LIMIT_3  0.228  -77.2%  0.063   0.832    0.025    *   \n",
       "59         ROLLING_PAY_PROP_2  0.997   -0.3%  0.995   1.000    0.043    *   \n",
       "60  ROLLING_OUTBAL_TO_LIMIT_2  6.481  548.1%  1.549  27.108     0.01    *   \n",
       "\n",
       "    dummy  importance  \n",
       "1    True       1.069  \n",
       "2    True       1.017  \n",
       "3    True       1.065  \n",
       "4    True       3.031  \n",
       "5    True       1.228  \n",
       "6    True       1.259  \n",
       "7    True       1.362  \n",
       "8    True       1.368  \n",
       "9    True       1.655  \n",
       "10   True       1.709  \n",
       "11   True       1.875  \n",
       "12   True       2.173  \n",
       "13   True       1.967  \n",
       "14   True       2.137  \n",
       "15   True       2.174  \n",
       "16   True       2.189  \n",
       "17   True       2.120  \n",
       "18   True       3.270  \n",
       "19   True       2.399  \n",
       "20   True       1.731  \n",
       "21   True       2.913  \n",
       "22   True       2.816  \n",
       "23   True       2.183  \n",
       "24   True       3.071  \n",
       "25   True       2.919  \n",
       "26   True       3.911  \n",
       "27   True       3.411  \n",
       "28  False       1.005  \n",
       "29  False       1.768  \n",
       "30  False       1.113  \n",
       "31  False       1.049  \n",
       "32  False       1.028  \n",
       "33  False       1.049  \n",
       "34  False       1.018  \n",
       "35  False       1.000  \n",
       "36  False       1.000  \n",
       "37  False       1.000  \n",
       "38  False       1.000  \n",
       "39  False       1.000  \n",
       "40  False       1.000  \n",
       "41  False       1.000  \n",
       "42  False       1.000  \n",
       "43  False       1.000  \n",
       "44  False       1.000  \n",
       "45  False       1.000  \n",
       "46  False       1.000  \n",
       "47  False       1.225  \n",
       "48  False       1.651  \n",
       "49  False       1.189  \n",
       "50  False       3.374  \n",
       "51  False       1.018  \n",
       "52  False       2.012  \n",
       "53  False       1.006  \n",
       "54  False       1.963  \n",
       "55  False       1.002  \n",
       "56  False       1.145  \n",
       "57  False       1.004  \n",
       "58  False       4.383  \n",
       "59  False       1.003  \n",
       "60  False       6.481  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.or_ci(lr, importance=True).head(60)  #.sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d52dc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>OR</th>\n",
       "      <th>OR%</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>p.values</th>\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>ROLLING_PAY_PROP_1</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.251</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ROLLING_OUTBAL_TO_LIMIT_1</td>\n",
       "      <td>0.762</td>\n",
       "      <td>-23.8%</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0.535</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>1.312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        index     OR     OR%   2.5%  97.5% p.values     dummy  \\\n",
       "61         ROLLING_PAY_PROP_1  1.008    0.8%  0.995  1.021    0.251     False   \n",
       "62  ROLLING_OUTBAL_TO_LIMIT_1  0.762  -23.8%  0.323  1.800    0.535     False   \n",
       "\n",
       "    importance  \n",
       "61       1.008  \n",
       "62       1.312  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.or_ci(lr, importance=True).tail(2)  #.sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "023fd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pseudo R-squared (McFadden): 0.127\n",
      "Pseudo R-squared (McFadden adjusted): 0.122\n",
      "Log-likelihood: -11073.348, AIC: 22272.696, BIC: 22782.102\n",
      "Chi-squared: 29067.69 df(62), p.value < 0.001 \n",
      "Nr obs: 24,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rsm.model_fit(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e6369d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyrsm import xtile\n",
    "from pyrsm.utils import ifelse\n",
    "from sklearn import metrics\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65adbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtile(x, n=5, rev=False):\n",
    "    \"\"\"\n",
    "    Split a numeric series into a number of bins and return a series of bin numbers\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : List, pandas series, or numpy array\n",
    "        Numeric variable to bin\n",
    "    n :\tint\n",
    "        Number of bins to create\n",
    "    rev\t: bool\n",
    "        Reverse the order of the bin numbers (False or True)\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy array with bin numbers for each numeric value in x\n",
    "    Examples\n",
    "    --------\n",
    "    xtile(np.arange(10), 5)\n",
    "    xtile(np.arange(10), 5, rev=True)\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    #breaks = np.quantile(x[np.isnan(x) == False], np.arange(0, n + 1) / n)\n",
    "    breaks = np.quantile(x, np.arange(0, n + 1) / n)\n",
    "    if len(np.unique(breaks)) == len(breaks):\n",
    "        bins = pd.cut(x, breaks, include_lowest=True, labels=False) + 1\n",
    "    else:\n",
    "        bins = bincode(x, breaks)\n",
    "\n",
    "    if rev is True:\n",
    "        bins = (n + 1) - bins\n",
    "\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b84d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qnt(df, rvar, lev, pred, qnt=10):\n",
    "    \"\"\"\n",
    "    Create quantiles and calculate input to use for lift and gains charts\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "    rvar : str\n",
    "        Name of the response variable column in df\n",
    "    lev : str\n",
    "        Name of the 'success' level in rvar\n",
    "    pred : str\n",
    "        Name of the column in df with model predictions\n",
    "    qnt : int\n",
    "        Number of quantiles to create\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe\n",
    "        Response metrics per quantile. Used as input for lift and gains charts\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.loc[:, (rvar, pred)]\n",
    "    df[\"bins\"] = xtile(df[pred], qnt)\n",
    "    df[\"rvar_int\"] = np.where(df[rvar] == lev, 1, 0)\n",
    "    perf_df = df.groupby(\"bins\").rvar_int.agg(nr_obs=\"count\", nr_resp=sum).reset_index()\n",
    "\n",
    "    # flip if needed\n",
    "    if perf_df.nr_resp.iloc[1] < perf_df.nr_resp.iloc[-1]:\n",
    "        perf_df = perf_df.sort_values(\"bins\", ascending=False)\n",
    "\n",
    "    perf_df[\"cum_obs\"] = np.cumsum(perf_df.nr_obs)\n",
    "    perf_df[\"cum_prop\"] = perf_df.cum_obs / perf_df.cum_obs.iloc[-1]\n",
    "    perf_df[\"cum_resp\"] = np.cumsum(perf_df.nr_resp)\n",
    "    return perf_df\n",
    "\n",
    "\n",
    "def gains_tab(df, rvar, lev, pred, qnt=10):\n",
    "    \"\"\"\n",
    "    Calculate cumulative gains using the cum_resp column\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "    rvar : str\n",
    "        Name of the response variable column in df\n",
    "    lev : str\n",
    "        Name of the 'success' level in rvar\n",
    "    pred : str\n",
    "        Name of the column in df with model predictions\n",
    "    qnt : int\n",
    "        Number of quantiles to create\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas dataframe\n",
    "        Gains measures per quantile. Input for gains chart\n",
    "    \"\"\"\n",
    "\n",
    "    df = calc_qnt(df, rvar, lev, pred, qnt=qnt)\n",
    "    df[\"cum_gains\"] = df.cum_resp / df.cum_resp.iloc[-1]\n",
    "    df0 = pd.DataFrame({\"cum_prop\": [0], \"cum_gains\": [0]})\n",
    "    df = pd.concat([df0, df], sort=False).reset_index(drop=True)\n",
    "    df.index = range(df.shape[0])\n",
    "    return df[[\"cum_prop\", \"cum_gains\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3bcd9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gains_plot(df, rvar, lev, pred, qnt=10, marker=\"o\", **kwargs):\n",
    "    \"\"\"\n",
    "    Plot a cumulative gains curve\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe or a dictionary of dataframes with keys to show multiple curves for different models or data samples\n",
    "    rvar : str\n",
    "        Name of the response variable column in df\n",
    "    lev : str\n",
    "        Name of the 'success' level in rvar\n",
    "    pred : str\n",
    "        Name of the column in df with model predictions\n",
    "    qnt : int\n",
    "        Number of quantiles to create\n",
    "    cost : int\n",
    "        Cost of an action\n",
    "    margin : int\n",
    "        Benefit of an action if a successful outcome results from the action\n",
    "    marker : str\n",
    "        Marker to use for line plot\n",
    "    **kwargs : Named arguments to be passed to the seaborn lineplot function\n",
    "    Returns\n",
    "    -------\n",
    "    Seaborn object\n",
    "        Plot of gaines per quantile\n",
    "    Examples\n",
    "    --------\n",
    "    gains_plot(df, \"buyer\", \"yes\", \"pred_a\")\n",
    "    gains_plot(df, \"buyer\", \"yes\", [\"pred_a\", \"pred_b\"], qnt=20)\n",
    "    dct = {\"Training\": df.query(\"training == 1\"), \"Test\": df.query(\"training == 0\")}\n",
    "    gains_plot(dct, \"buyer\", \"yes\", \"pred_a\")\n",
    "    \"\"\"\n",
    "    dct = ifelse(type(df) is dict, df, {\"\": df})\n",
    "    pred = ifelse(type(pred) is list, pred, [pred])\n",
    "    group = ifelse(len(pred) > 1 or len(dct.keys()) > 1, \"predictor\", None)\n",
    "    rd = [\n",
    "        gains_tab(dct[k], rvar, lev, p, qnt=qnt).assign(\n",
    "            predictor=p + ifelse(k == \"\", k, f\" ({k})\")\n",
    "        )\n",
    "        for k in dct.keys()\n",
    "        for p in pred\n",
    "    ]\n",
    "    rd = pd.concat(rd).reset_index(drop=True)\n",
    "    fig = sns.lineplot(\n",
    "        x=\"cum_prop\", y=\"cum_gains\", data=rd, hue=group, marker=marker, **kwargs\n",
    "    )\n",
    "    fig.set(ylabel=\"Cumulative gains\", xlabel=\"Proportion of customers\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
    "    if len(dct) > 1:\n",
    "        fig.legend(title=None)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578040e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39a1144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dat = df[['ID', 'DEFAULT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a883149",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dat = eval_dat.assign(predicted_prob_lr = lr.predict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95f5969c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Proportion of customers', ylabel='Cumulative gains'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dct = {\"training\": eval_dat.iloc[train_index], \"test\": eval_dat.iloc[test_index]}\n",
    "rsm.gains_plot(dct, rvar = 'DEFAULT', pred = 'predicted_prob_lr', lev = 1, qnt = 10, marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2027952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training':           ID  DEFAULT  predicted_prob_lr\n",
       " 9648    9649        0           0.126210\n",
       " 15895  15896        0           0.337030\n",
       " 7324    7325        0           0.263812\n",
       " 26043  26044        1           0.538578\n",
       " 12134  12135        0           0.129148\n",
       " ...      ...      ...                ...\n",
       " 17249  17250        1           0.226299\n",
       " 19534  19535        0           0.274742\n",
       " 5257    5258        0           0.108005\n",
       " 2621    2622        0           0.237210\n",
       " 14906  14907        0           0.317682\n",
       " \n",
       " [24000 rows x 3 columns],\n",
       " 'test':           ID  DEFAULT  predicted_prob_lr\n",
       " 21515  21516        0           0.384894\n",
       " 22062  22063        0           0.071483\n",
       " 9191    9192        0           0.229462\n",
       " 10962  10963        1           0.121402\n",
       " 2192    2193        0           0.149959\n",
       " ...      ...      ...                ...\n",
       " 20311  20312        0           0.264959\n",
       " 29642  29643        1           0.457793\n",
       " 15527  15528        0           0.173904\n",
       " 22162  22163        0           0.267965\n",
       " 28965  28966        1           0.097083\n",
       " \n",
       " [6000 rows x 3 columns]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa172d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>predictor</th>\n",
       "      <th>total</th>\n",
       "      <th>AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>616</td>\n",
       "      <td>207</td>\n",
       "      <td>18484</td>\n",
       "      <td>4693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>148</td>\n",
       "      <td>59</td>\n",
       "      <td>4614</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type          predictor  total   AUC  accuracy   TP   FP     TN    FN\n",
       "0  training  predicted_prob_lr  24000  0.73      0.80  616  207  18484  4693\n",
       "1      test  predicted_prob_lr   6000  0.73      0.79  148   59   4614  1179"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.evalbin(dct, rvar = 'DEFAULT', lev = 1, pred = ['predicted_prob_lr'], cost = 3, margin=5, dec=2)[['Type', 'predictor', 'total', 'AUC', 'accuracy', 'TP', 'FP', 'TN', 'FN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6a0c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Kolmogarov-smirnov (KS intuition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "137246be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='PAY_1', ylabel='Count'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEHCAYAAABvHnsJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVaklEQVR4nO3df7BfdX3n8ecLUn5pISh3mDRhN9kxYxcZW9mIKLtOl1gI1DV0BhyYXQlp2sxu0aJ2aqH+wYzKjE5dUVukzQAVWwoixQErGlNEu50CEn4MCki5AwWSvcjVAHaKysa894/vJ/htvIGbk/v9fnPzfT5mvnPPeZ/POedzArmvnF+fb6oKSZK6OGDUHZAkzV+GiCSpM0NEktSZISJJ6swQkSR1tmDUHRi2o446qpYuXTrqbkjSvHL33Xd/v6omdq2PXYgsXbqUzZs3j7obkjSvJHl8prqXsyRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjobu5cNNTs7duxgamoKgEWLFnHAAf57Q9LP8zeDZjQ1NcXayzay9rKNL4aJJO3KMxHt1iFHHDXqLkjax3kmIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1NnAQiTJVUmeTvKdvtofJ/lukvuTfDHJwr5lFyWZTPJwklP76qtabTLJhX31ZUnubPXPJzloUMciSZrZIM9EPgus2qW2CTiuql4P/BNwEUCSY4Gzgde1dT6T5MAkBwKXAacBxwLntLYAHwMurarXAM8A6wZ4LJKkGQwsRKrq74Ftu9S+VlXb2+wdwJI2vRq4rqp+UlWPAZPACe0zWVWPVtULwHXA6iQBTgZuaOtfDZwxqGORJM1slPdEfgv4SpteDDzZt2xLq+2u/mrg2b5A2lmfUZL1STYn2Tw9PT1H3ZckjSREknwQ2A5cM4z9VdWGqlpRVSsmJiaGsUtJGgtDH8U3yXnA24GVVVWtvBU4pq/ZklZjN/UfAAuTLGhnI/3tJUlDMtQzkSSrgA8A76iq5/sW3QycneTgJMuA5cC3gLuA5e1JrIPo3Xy/uYXPbcCZbf01wE3DOg5JUs8gH/G9FrgdeG2SLUnWAX8K/CKwKcl9Sf4MoKoeAK4HHgS+CpxfVT9tZxnvBjYCDwHXt7YAfwi8P8kkvXskVw7qWCRJMxvY5ayqOmeG8m5/0VfVJcAlM9RvAW6Zof4ovae3JEkj4hvrkqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjobWIgkuSrJ00m+01d7VZJNSR5pP49s9ST5dJLJJPcnOb5vnTWt/SNJ1vTV/1OSb7d1Pp0kgzoWSdLMBnkm8llg1S61C4Fbq2o5cGubBzgNWN4+64HLoRc6wMXAm4ATgIt3Bk9r8zt96+26L0nSgA0sRKrq74Ftu5RXA1e36auBM/rqn6ueO4CFSRYBpwKbqmpbVT0DbAJWtWWHV9UdVVXA5/q2JUkakmHfEzm6qqba9FPA0W16MfBkX7strfZS9S0z1CVJQzSyG+vtDKKGsa8k65NsTrJ5enp6GLuUpLEw7BD5XrsURfv5dKtvBY7pa7ek1V6qvmSG+oyqakNVraiqFRMTE3t9EJKknmGHyM3Azies1gA39dXPbU9pnQg81y57bQROSXJku6F+CrCxLfthkhPbU1nn9m1LkjQkCwa14STXAr8GHJVkC72nrD4KXJ9kHfA48M7W/BbgdGASeB5YC1BV25J8GLirtftQVe28Wf+79J4AOxT4SvtIkoZoYCFSVefsZtHKGdoWcP5utnMVcNUM9c3AcXvTR0nS3vGNdUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM5GEiJJ3pfkgSTfSXJtkkOSLEtyZ5LJJJ9PclBre3Cbn2zLl/Zt56JWfzjJqaM4FkkaZ0MPkSSLgd8DVlTVccCBwNnAx4BLq+o1wDPAurbKOuCZVr+0tSPJsW291wGrgM8kOXCYxyJJ425Ul7MWAIcmWQAcBkwBJwM3tOVXA2e06dVtnrZ8ZZK0+nVV9ZOqegyYBE4YTvclSTCCEKmqrcDHgSfohcdzwN3As1W1vTXbAixu04uBJ9u621v7V/fXZ1jn30iyPsnmJJunp6fn9oAkaYyN4nLWkfTOIpYBvwS8gt7lqIGpqg1VtaKqVkxMTAxyV5I0VkZxOettwGNVNV1V/w+4ETgJWNgubwEsAba26a3AMQBt+RHAD/rrM6wjSRqCUYTIE8CJSQ5r9zZWAg8CtwFntjZrgJva9M1tnrb861VVrX52e3prGbAc+NaQjkGSRO8G91BV1Z1JbgDuAbYD9wIbgC8D1yX5SKtd2Va5EvjLJJPANnpPZFFVDyS5nl4AbQfOr6qfDvVgJGnMDT1EAKrqYuDiXcqPMsPTVVX1Y+Cs3WznEuCSOe+gJGlWfGNdktSZISJJ6mxWIZLkpNnUJEnjZbZnIn8yy5okaYy85I31JG8G3gJMJHl/36LD6Y15JUkaYy/3dNZBwCtbu1/sq/+Qn73TIUkaUy8ZIlX1TeCbST5bVY8PqU+SpHlitu+JHJxkA7C0f52qOnkQnZIkzQ+zDZEvAH8GXAH4VrgkCZh9iGyvqssH2hNJ0rwz20d8v5Tkd5MsSvKqnZ+B9kyStM+b7ZnIzlF0/6CvVsB/mNvuSJLmk1mFSFUtG3RHJEnzz6xCJMm5M9Wr6nNz2x1J0nwy28tZb+ybPoTeF0ndAxgikjTGZns56z3980kWAtcNokOSpPmj61Dw/wp4n0SSxtxs74l8id7TWNAbePE/AtcPqlOSpPlhtvdEPt43vR14vKq2DKA/kqR5ZFaXs9pAjN+lN5LvkcALg+yUJGl+mO03G74T+BZwFvBO4M4kDgUvSWNutpezPgi8saqeBkgyAfwdcMOgOiZJ2vfN9umsA3YGSPODPVhXkrSfmm0QfDXJxiTnJTkP+DJwS9edJlmY5IYk303yUJI3t0EdNyV5pP08srVNkk8nmUxyf5Lj+7azprV/JMma3e9RkjQILxkiSV6T5KSq+gPgz4HXt8/twIa92O+ngK9W1S8DvwI8BFwI3FpVy4Fb2zzAacDy9lkPXN769irgYuBNwAnAxTuDR5I0HC93JvJJet+nTlXdWFXvr6r3A19sy/ZYkiOAtwJXtu2+UFXPAquBq1uzq4Ez2vRq4HPVcwewMMki4FRgU1Vtq6pngE3Aqi59kiR183IhcnRVfXvXYqst7bjPZcA08BdJ7k1yRZJXtH1NtTZPAUe36cXAk33rb2m13dV/TpL1STYn2Tw9Pd2x25KkXb1ciCx8iWWHdtznAuB44PKqegO9IVQu7G9QVcXP3pDfa1W1oapWVNWKiYmJudqsJI29lwuRzUl+Z9dikt8G7u64zy3Alqq6s83fQC9UvtcuU9F+7nwabCtwTN/6S1ptd3VJ0pC8XIi8F1ib5BtJ/nf7fBNYB1zQZYdV9RTwZJLXttJK4EHgZn72DYprgJva9M3Aue0prROB59plr43AKUmObDfUT2k1SdKQvOTLhlX1PeAtSf4rcFwrf7mqvr6X+30PcE2Sg4BHgbX0Au36JOuAx+m9GQ+9R4lPByaB51tbqmpbkg8Dd7V2H6qqbXvZL0nSHpjt94ncBtw2VzutqvuAFTMsWjlD2wLO3812rgKumqt+SZL2jG+dS5I6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLU2ayGgtfo7Nixg6mp3lfPL1q0iAMOMPcl7Tv8jbSPm5qaYu1lG1l72cYXw0SS9hWeicwDhxxx1Ki7IEkz8kxEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTORhYiSQ5Mcm+Sv23zy5LcmWQyyeeTHNTqB7f5ybZ8ad82Lmr1h5OcOqJDkaSxNcozkQuAh/rmPwZcWlWvAZ4B1rX6OuCZVr+0tSPJscDZwOuAVcBnkhw4pL5LkhhRiCRZAvwGcEWbD3AycENrcjVwRpte3eZpy1e29quB66rqJ1X1GDAJnDCUA5AkAaM7E/kk8AFgR5t/NfBsVW1v81uAxW16MfAkQFv+XGv/Yn2Gdf6NJOuTbE6yeXp6eg4PQ5LG29BDJMnbgaer6u5h7bOqNlTViqpaMTExMazdStJ+bxRjZ50EvCPJ6cAhwOHAp4CFSRa0s40lwNbWfitwDLAlyQLgCOAHffWd+teRJA3B0M9EquqiqlpSVUvp3Rj/elX9d+A24MzWbA1wU5u+uc3Tln+9qqrVz25Pby0DlgPfGtJhSJLYt0bx/UPguiQfAe4Frmz1K4G/TDIJbKMXPFTVA0muBx4EtgPnV9VPh99tSRpfIw2RqvoG8I02/SgzPF1VVT8GztrN+pcAlwyuh5Kkl+Ib65KkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqbORfsf6fLJjxw6mpqYAWLRoEQccYP7ONf+MpfnHv6WzNDU1xdrLNrL2so0v/qLT3PLPWJp/PBPZA4cccdSou7Df889Yml88E5EkdTb0EElyTJLbkjyY5IEkF7T6q5JsSvJI+3lkqyfJp5NMJrk/yfF921rT2j+SZM2wj0WSxt0ozkS2A79fVccCJwLnJzkWuBC4taqWA7e2eYDTgOXtsx64HHqhA1wMvAk4Abh4Z/BIkoZj6CFSVVNVdU+b/hfgIWAxsBq4ujW7GjijTa8GPlc9dwALkywCTgU2VdW2qnoG2ASsGt6RSJJGek8kyVLgDcCdwNFVtfORnKeAo9v0YuDJvtW2tNru6pKkIRlZiCR5JfA3wHur6of9y6qqgJrDfa1PsjnJ5unp6bnarCSNvZGESJJfoBcg11TVja38vXaZivbz6VbfChzTt/qSVttd/edU1YaqWlFVKyYmJubuQCRpzI3i6awAVwIPVdUn+hbdDOx8wmoNcFNf/dz2lNaJwHPtstdG4JQkR7Yb6qe0miRpSEbxsuFJwLuAbye5r9X+CPgocH2SdcDjwDvbsluA04FJ4HlgLUBVbUvyYeCu1u5DVbVtKEcgSQJGECJV9Q9AdrN45QztCzh/N9u6Crhq7nonSdoTvrEuSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzkYxFLy0T9qxYwdTU71vaF60aBEHHOC/saSX498SqZmammLtZRtZe9nGF8NE0kvzTETqc8gRR426C9K84pmIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ/M+RJKsSvJwkskkF466P5I0TuZ1iCQ5ELgMOA04FjgnybGj7ZW0Z3bs2MHWrVvZunUrO3bsGHV3pD0y34c9OQGYrKpHAZJcB6wGHhzEzn783PcBhjqu0tTU1Njsd1THOur9T01N8f7PfgOAT5z3ayxatGho+9b4WLx48UC2m6oayIaHIcmZwKqq+u02/y7gTVX17l3arQfWt9nXAg933OVRwPc7rjtfeczjYdyOedyOF/b+mP99VU3sWpzvZyKzUlUbgA17u50km6tqxRx0ad7wmMfDuB3zuB0vDO6Y5/U9EWArcEzf/JJWkyQNwXwPkbuA5UmWJTkIOBu4ecR9kqSxMa8vZ1XV9iTvBjYCBwJXVdUDA9zlXl8Sm4c85vEwbsc8bscLAzrmeX1jXZI0WvP9cpYkaYQMEUlSZ4bIHkryx0m+m+T+JF9MsnDUfRqEcRtOJskxSW5L8mCSB5JcMOo+DUuSA5Pcm+RvR92XYUiyMMkN7e/xQ0nePOo+DVqS97X/r7+T5Nokh8zVtg2RPbcJOK6qXg/8E3DRiPsz58Z0OJntwO9X1bHAicD5Y3DMO10APDTqTgzRp4CvVtUvA7/Cfn7sSRYDvwesqKrj6D2EdPZcbd8Q2UNV9bWq2t5m76D3bsr+5sXhZKrqBWDncDL7raqaqqp72vS/0PvFMphxIvYhSZYAvwFcMeq+DEOSI4C3AlcCVNULVfXsSDs1HAuAQ5MsAA4D/u9cbdgQ2Tu/BXxl1J0YgMXAk33zWxiDX6g7JVkKvAG4c8RdGYZPAh8AxmXkx2XANPAX7RLeFUleMepODVJVbQU+DjwBTAHPVdXX5mr7hsgMkvxdu3a462d1X5sP0rsEcs3oeqq5luSVwN8A762qH466P4OU5O3A01V196j7MkQLgOOBy6vqDcC/Avv1Pb8kR9K7krAM+CXgFUn+x1xtf16/bDgoVfW2l1qe5Dzg7cDK2j9ftBnL4WSS/AK9ALmmqm4cdX+G4CTgHUlOBw4BDk/yV1U1Z79g9kFbgC1VtfMs8wb28xAB3gY8VlXTAEluBN4C/NVcbNwzkT2UZBW90/93VNXzo+7PgIzdcDJJQu86+UNV9YlR92cYquqiqlpSVUvp/Tf++n4eIFTVU8CTSV7bSisZ0FdH7EOeAE5Mclj7/3wlc/gwgWcie+5PgYOBTb3/HtxRVf9ztF2aWyMYTmZfcBLwLuDbSe5rtT+qqltG1yUNyHuAa9o/kB4F1o64PwNVVXcmuQG4h94l+HuZwyFQHPZEktSZl7MkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEGIMlPk9zXhsv5QpLDWn1BkukkH23zv57k9vYSWP+w7G/ZzXbfmuSeJNuTnDm8I5JmZohIg/GjqvrVNvT2C8DOF1J/nd5XCJyVJFW1CXgcWNeWvwfYXFX/uJvtPgGcB/z1wHou7QHfWJcG7/8Ar2/T59D7Pov/BbwZ+EfgfcA/JLkdeDe9ofhnVFX/DJBkXEbd1T7OMxFpgNr3N5xGbziVQ+gNhvcl4Fp6gUJVTdEbkv124CNVtW00vZX2nCEiDcahbQyuzfQuQV1Jb+Tn26rqR/RGCz6jfYsk9L5J8sCq+uwI+ip15uUsaTB+VFW/2l9Icg7wn5P8cyu9GjgZ2FRVO5I4kJ3mHc9EpCFIcjjwX4B/V1VL2/Dr59MuaUnzlSEiDcdv0vu+jp/01W4C/luSg2e7kSRvTLIFOAv48yT7+xD92sc5FLwkqTPPRCRJnXljXdoHJfkgvUtW/b5QVZeMoj/S7ng5S5LUmZezJEmdGSKSpM4MEUlSZ4aIJKmz/w/glzktS/KgpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df.query('DEFAULT == 0')['PAY_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58086b0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='PAY_1', ylabel='Count'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df.query('DEFAULT == 1')['PAY_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "089ebcc5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21122239342578325"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('DEFAULT == 0')['PAY_1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6de229ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6681735985533453"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('DEFAULT == 1')['PAY_1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f658b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ea29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection wrapper:\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nfeatures = len(Xs.columns)\n",
    "clf = RandomForestClassifier(n_estimators=5, n_jobs=-1)\n",
    "sfs = SFS(clf,k_features=nfeatures,forward=True,verbose=2,scoring='roc_auc',cv=2,n_jobs=-1)\n",
    "# sfs = SFS(clf,k_features=1,forward=False,verbose=2,scoring=fdr,cv=2,n_jobs=-1)\n",
    "sfs.fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb94597",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_FS = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "vars_FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_top_30 = list(vars_FS.loc[30, 'feature_names'])\n",
    "sfs_top_30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc6fdf",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70405a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ffd3964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.7720604064846245\n",
      "CPU times: user 27min 35s, sys: 0 ns, total: 27min 35s\n",
      "Wall time: 27min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "param_grid_rf = {\"n_estimators\": [100, 200, 300, 400, 500], 'criterion': ['gini', 'entropy']}\n",
    "\n",
    "cv_rf = GridSearchCV(clf_rf, param_grid_rf, cv = 5, scoring = 'roc_auc')\n",
    "\n",
    "cv_rf.fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])\n",
    "\n",
    "print(cv_rf.best_params_)\n",
    "print(cv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #took long time to do gridsearch cv:\n",
    "\n",
    "# print(cv_rf.best_params_)\n",
    "# print(cv_rf.best_score_)\n",
    "\n",
    "# result:\n",
    "# {'criterion': 'entropy', 'n_estimators': 500}\n",
    "# 0.772366450848367   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit * probabality >= cost\n",
    "\n",
    "# profit from keeping on a customer and not declaring delinquint * profit >= cost of that customer defaulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368acb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd3a2f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 0 ns, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_rf = RandomForestClassifier(criterion = cv_rf.best_params_['criterion'], n_estimators = cv_rf.best_params_['n_estimators'], oob_score = True, random_state = 1234).fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "affda383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure order of categorical response is correct, expect [0: no, 1: default]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Making sure order of categorical response is correct, expect [0: no, 1: default]\")\n",
    "print(model_rf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "999919ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DEFAULT</th>\n",
       "      <th>predicted_prob_lr</th>\n",
       "      <th>predicted_prob_rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>0.614973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162715</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198494</td>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235322</td>\n",
       "      <td>0.180233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093034</td>\n",
       "      <td>0.094059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165336</td>\n",
       "      <td>0.122905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117220</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.836941</td>\n",
       "      <td>0.464481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>0.218182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237015</td>\n",
       "      <td>0.162791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  DEFAULT  predicted_prob_lr predicted_prob_rf\n",
       "0          1        1           0.630370          0.614973\n",
       "1          2        1           0.162715              0.32\n",
       "2          3        0           0.198494          0.113636\n",
       "3          4        0           0.235322          0.180233\n",
       "4          5        0           0.093034          0.094059\n",
       "...      ...      ...                ...               ...\n",
       "29995  29996        0           0.165336          0.122905\n",
       "29996  29997        0           0.117220              0.12\n",
       "29997  29998        1           0.836941          0.464481\n",
       "29998  29999        1           0.208816          0.218182\n",
       "29999  30000        1           0.237015          0.162791\n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dat['predicted_prob_rf'] = None\n",
    "\n",
    "eval_dat.loc[train_index, 'predicted_prob_rf'] = model_rf.oob_decision_function_[:, 1]\n",
    "eval_dat.loc[test_index, 'predicted_prob_rf'] = model_rf.predict_proba(Xs.iloc[test_index])[:, 1]\n",
    "\n",
    "eval_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1096d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Proportion of customers', ylabel='Cumulative gains'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dct = {\"training\": eval_dat.iloc[train_index], \"test\": eval_dat.iloc[test_index]}\n",
    "gains_plot(dct, rvar = 'DEFAULT', pred = 'predicted_prob_rf', lev = 1, qnt = 10, marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "016e65a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>predictor</th>\n",
       "      <th>total</th>\n",
       "      <th>AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>616</td>\n",
       "      <td>207</td>\n",
       "      <td>18484</td>\n",
       "      <td>4693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1349</td>\n",
       "      <td>546</td>\n",
       "      <td>18145</td>\n",
       "      <td>3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>148</td>\n",
       "      <td>59</td>\n",
       "      <td>4614</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>325</td>\n",
       "      <td>146</td>\n",
       "      <td>4527</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type          predictor  total   AUC  accuracy    TP   FP     TN    FN\n",
       "0  training  predicted_prob_lr  24000  0.73      0.80   616  207  18484  4693\n",
       "1  training  predicted_prob_rf  24000  0.77      0.81  1349  546  18145  3960\n",
       "2      test  predicted_prob_lr   6000  0.73      0.79   148   59   4614  1179\n",
       "3      test  predicted_prob_rf   6000  0.77      0.81   325  146   4527  1002"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#60% threshold\n",
    "rsm.evalbin(dct, rvar = 'DEFAULT', lev = 1, pred = ['predicted_prob_lr', 'predicted_prob_rf'], cost = 3, margin=5, dec=2)[['Type', 'predictor', 'total', 'AUC', 'accuracy', 'TP', 'FP', 'TN', 'FN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7699f5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>predictor</th>\n",
       "      <th>total</th>\n",
       "      <th>AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1296</td>\n",
       "      <td>552</td>\n",
       "      <td>18139</td>\n",
       "      <td>4013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1962</td>\n",
       "      <td>1022</td>\n",
       "      <td>17669</td>\n",
       "      <td>3347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.81</td>\n",
       "      <td>323</td>\n",
       "      <td>131</td>\n",
       "      <td>4542</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>481</td>\n",
       "      <td>262</td>\n",
       "      <td>4411</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type          predictor  total   AUC  accuracy    TP    FP     TN    FN\n",
       "0  training  predicted_prob_lr  24000  0.73      0.81  1296   552  18139  4013\n",
       "1  training  predicted_prob_rf  24000  0.77      0.82  1962  1022  17669  3347\n",
       "2      test  predicted_prob_lr   6000  0.73      0.81   323   131   4542  1004\n",
       "3      test  predicted_prob_rf   6000  0.77      0.82   481   262   4411   846"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#50% threshold\n",
    "rsm.evalbin(dct, rvar = 'DEFAULT', lev = 1, pred = ['predicted_prob_lr', 'predicted_prob_rf'], cost = 1, margin=2, dec=2)[['Type', 'predictor', 'total', 'AUC', 'accuracy', 'TP', 'FP', 'TN', 'FN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325050e9",
   "metadata": {},
   "source": [
    "## XG Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "371ff897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a04e07a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [3, 4, 5, 6], 'n_estimators': [4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model tended to overfit on train data, reduced n_estimators \n",
    "clf_xgb = xgb.XGBClassifier()\n",
    "\n",
    "param_grid_xgb = {\n",
    "\n",
    "    \"max_depth\": list(range(3, 7)), #cross validation has been completed with a range of hyper parameters to select one with highest AUC\n",
    "    \"n_estimators\": list(range(4, 10, 1)), #cross validation has been completed with a range of hyper parameters to select one with highest AUC\n",
    "}\n",
    "scoring = {\"AUC\": \"roc_auc\"}\n",
    "param_grid_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2018c7dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:15] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:50] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:50] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:15] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:40] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 5min 45s, sys: 18.5 s, total: 6min 3s\n",
      "Wall time: 1min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'max_depth': [3, 4, 5, 6],\n",
       "                         'n_estimators': [4, 5, 6, 7, 8, 9]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cv_xgb = GridSearchCV(clf_xgb, param_grid_xgb, cv = 5, scoring = 'roc_auc')\n",
    "cv_xgb.fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7b8d4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 9}\n",
      "0.7777991904781812\n"
     ]
    }
   ],
   "source": [
    "print(cv_xgb.best_params_)\n",
    "print(cv_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c5131291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.36 s, sys: 147 ms, total: 3.51 s\n",
      "Wall time: 919 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(max_depth = cv_xgb.best_params_['max_depth'], n_estimators = cv_xgb.best_params_['n_estimators'], random_state = 1234, use_label_encoder = False, objective = 'binary:logistic').fit(np.array(Xs.iloc[train_index]), np.array(df['DEFAULT'].iloc[train_index]), eval_metric='logloss')\n",
    "#model_xgb = xgb.XGBClassifier(random_state = 1234, n_estimators = 5, use_label_encoder = False, objective = 'binary:logistic').fit(np.array(Xs.iloc[train_index]), np.array(df['DEFAULT'].iloc[train_index]), eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "64cec868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4e90cb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure order of categorical response is correct, expect [0: no, 1: default]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Making sure order of categorical response is correct, expect [0: no, 1: default]\")\n",
    "print(model_xgb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0c0cc099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DEFAULT</th>\n",
       "      <th>predicted_prob_lr</th>\n",
       "      <th>predicted_prob_rf</th>\n",
       "      <th>predicted_prob_xgb</th>\n",
       "      <th>predicted_prob_nn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>0.614973</td>\n",
       "      <td>0.676574</td>\n",
       "      <td>0.638063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162715</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.453784</td>\n",
       "      <td>0.283493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198494</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.144193</td>\n",
       "      <td>0.120789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235322</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>0.178516</td>\n",
       "      <td>0.180855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093034</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>0.07559</td>\n",
       "      <td>0.088423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165336</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.15629</td>\n",
       "      <td>0.108045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117220</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.135859</td>\n",
       "      <td>0.138451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.836941</td>\n",
       "      <td>0.464481</td>\n",
       "      <td>0.65373</td>\n",
       "      <td>0.790902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.144392</td>\n",
       "      <td>0.167799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237015</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.180134</td>\n",
       "      <td>0.186671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  DEFAULT  predicted_prob_lr predicted_prob_rf predicted_prob_xgb  \\\n",
       "0          1        1           0.630370          0.614973           0.676574   \n",
       "1          2        1           0.162715              0.32           0.453784   \n",
       "2          3        0           0.198494          0.113636           0.144193   \n",
       "3          4        0           0.235322          0.180233           0.178516   \n",
       "4          5        0           0.093034          0.094059            0.07559   \n",
       "...      ...      ...                ...               ...                ...   \n",
       "29995  29996        0           0.165336          0.122905            0.15629   \n",
       "29996  29997        0           0.117220              0.12           0.135859   \n",
       "29997  29998        1           0.836941          0.464481            0.65373   \n",
       "29998  29999        1           0.208816          0.218182           0.144392   \n",
       "29999  30000        1           0.237015          0.162791           0.180134   \n",
       "\n",
       "      predicted_prob_nn  \n",
       "0              0.638063  \n",
       "1              0.283493  \n",
       "2              0.120789  \n",
       "3              0.180855  \n",
       "4              0.088423  \n",
       "...                 ...  \n",
       "29995          0.108045  \n",
       "29996          0.138451  \n",
       "29997          0.790902  \n",
       "29998          0.167799  \n",
       "29999          0.186671  \n",
       "\n",
       "[30000 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dat['predicted_prob_xgb'] = None\n",
    "\n",
    "eval_dat.loc[train_index, 'predicted_prob_xgb'] = model_xgb.predict_proba(Xs.iloc[train_index])[:, 1]\n",
    "eval_dat.loc[test_index, 'predicted_prob_xgb'] = model_xgb.predict_proba(Xs.iloc[test_index])[:, 1]\n",
    "\n",
    "eval_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f9bcec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Proportion of customers', ylabel='Cumulative gains'>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dct = {\"training\": eval_dat.iloc[train_index], \"test\": eval_dat.iloc[test_index]}\n",
    "gains_plot(dct, rvar = 'DEFAULT', pred = 'predicted_prob_xgb', lev = 1, qnt = 10, marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1fa937a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>predictor</th>\n",
       "      <th>total</th>\n",
       "      <th>AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>616</td>\n",
       "      <td>207</td>\n",
       "      <td>18484</td>\n",
       "      <td>4693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1349</td>\n",
       "      <td>546</td>\n",
       "      <td>18145</td>\n",
       "      <td>3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_xgb</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1804</td>\n",
       "      <td>432</td>\n",
       "      <td>18259</td>\n",
       "      <td>3505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>148</td>\n",
       "      <td>59</td>\n",
       "      <td>4614</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>325</td>\n",
       "      <td>146</td>\n",
       "      <td>4527</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_xgb</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>375</td>\n",
       "      <td>167</td>\n",
       "      <td>4506</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type           predictor  total   AUC  accuracy    TP   FP     TN    FN\n",
       "0  training   predicted_prob_lr  24000  0.73      0.80   616  207  18484  4693\n",
       "1  training   predicted_prob_rf  24000  0.77      0.81  1349  546  18145  3960\n",
       "2  training  predicted_prob_xgb  24000  0.82      0.84  1804  432  18259  3505\n",
       "3      test   predicted_prob_lr   6000  0.73      0.79   148   59   4614  1179\n",
       "4      test   predicted_prob_rf   6000  0.77      0.81   325  146   4527  1002\n",
       "5      test  predicted_prob_xgb   6000  0.78      0.81   375  167   4506   952"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.evalbin(dct, rvar = 'DEFAULT', lev = 1, pred = ['predicted_prob_lr', 'predicted_prob_rf', 'predicted_prob_xgb'], cost = 3, margin=5, dec=2)[['Type', 'predictor', 'total', 'AUC', 'accuracy', 'TP', 'FP', 'TN', 'FN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836c1c9",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5c3efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b68cd454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,), (2,), (3,), (4,), (1, 1), (2, 2), (3, 3), (4, 4)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_hnodes = range(1, 5) \n",
    "hls = list(zip(nr_hnodes)) + list(zip(nr_hnodes, nr_hnodes))\n",
    "#hls = (4,4) # selected 2 hidden layers with 4 nodes in each\n",
    "\n",
    "hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "67d25c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': ['tanh', 'relu'],\n",
       " 'hidden_layer_sizes': [(1,),\n",
       "  (2,),\n",
       "  (3,),\n",
       "  (4,),\n",
       "  (1, 1),\n",
       "  (2, 2),\n",
       "  (3, 3),\n",
       "  (4, 4)],\n",
       " 'alpha': [0.25, 0.5, 0.75, 1.0]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn = MLPClassifier()\n",
    "\n",
    "param_grid_nn = {\n",
    "    'activation': [\"tanh\", \"relu\"],\n",
    "    'hidden_layer_sizes': hls,\n",
    "    'alpha': [0.25, 0.5, 0.75, 1.0] #cross validation has been completed with a range of hyper parameters to select one with highest AUC\n",
    "}\n",
    "scoring = {'AUC': 'roc_auc'}\n",
    "param_grid_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a18f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'alpha': 0.25, 'hidden_layer_sizes': (4, 4)}\n",
      "0.7709698804945893\n",
      "CPU times: user 42min 40s, sys: 28min 25s, total: 1h 11min 5s\n",
      "Wall time: 35min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "cv_nn = GridSearchCV(clf_nn, param_grid_nn, cv = 5, scoring = 'roc_auc')\n",
    "cv_nn.fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])\n",
    "print(cv_nn.best_params_)\n",
    "print(cv_nn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91fb6ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 0 ns, total: 12.5 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(activation = cv_nn.best_params_['activation'], alpha=cv_nn.best_params_['alpha'], hidden_layer_sizes=cv_nn.best_params_['hidden_layer_sizes'], random_state = 1234).fit(Xs.iloc[train_index], df['DEFAULT'].iloc[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5e3755fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure order of categorical response is correct, expect [0: no, 1: default]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Making sure order of categorical response is correct, expect [0: no, 1: default]\")\n",
    "print(model_nn.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "069218df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DEFAULT</th>\n",
       "      <th>predicted_prob_lr</th>\n",
       "      <th>predicted_prob_rf</th>\n",
       "      <th>predicted_prob_xgb</th>\n",
       "      <th>predicted_prob_nn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>0.614973</td>\n",
       "      <td>0.912451</td>\n",
       "      <td>0.638063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162715</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.877826</td>\n",
       "      <td>0.283493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198494</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.140775</td>\n",
       "      <td>0.120789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.235322</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>0.143415</td>\n",
       "      <td>0.180855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093034</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>0.01382</td>\n",
       "      <td>0.088423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165336</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.108045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117220</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.255563</td>\n",
       "      <td>0.138451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.836941</td>\n",
       "      <td>0.464481</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.790902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.902361</td>\n",
       "      <td>0.167799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237015</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.594636</td>\n",
       "      <td>0.186671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  DEFAULT  predicted_prob_lr predicted_prob_rf predicted_prob_xgb  \\\n",
       "0          1        1           0.630370          0.614973           0.912451   \n",
       "1          2        1           0.162715              0.32           0.877826   \n",
       "2          3        0           0.198494          0.113636           0.140775   \n",
       "3          4        0           0.235322          0.180233           0.143415   \n",
       "4          5        0           0.093034          0.094059            0.01382   \n",
       "...      ...      ...                ...               ...                ...   \n",
       "29995  29996        0           0.165336          0.122905           0.034611   \n",
       "29996  29997        0           0.117220              0.12           0.255563   \n",
       "29997  29998        1           0.836941          0.464481             0.9471   \n",
       "29998  29999        1           0.208816          0.218182           0.902361   \n",
       "29999  30000        1           0.237015          0.162791           0.594636   \n",
       "\n",
       "      predicted_prob_nn  \n",
       "0              0.638063  \n",
       "1              0.283493  \n",
       "2              0.120789  \n",
       "3              0.180855  \n",
       "4              0.088423  \n",
       "...                 ...  \n",
       "29995          0.108045  \n",
       "29996          0.138451  \n",
       "29997          0.790902  \n",
       "29998          0.167799  \n",
       "29999          0.186671  \n",
       "\n",
       "[30000 rows x 6 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dat['predicted_prob_nn'] = None\n",
    "\n",
    "eval_dat.loc[train_index, 'predicted_prob_nn'] = model_nn.predict_proba(Xs.iloc[train_index])[:,1]\n",
    "eval_dat.loc[test_index, 'predicted_prob_nn'] = model_nn.predict_proba(Xs.iloc[test_index])[:,1]\n",
    "\n",
    "eval_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed133359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Proportion of customers', ylabel='Cumulative gains'>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dct = {\"training\": eval_dat.iloc[train_index], \"test\": eval_dat.iloc[test_index]}\n",
    "gains_plot(dct, rvar = 'DEFAULT', pred = 'predicted_prob_nn', lev = 1, qnt = 10, marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7c5a5639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>predictor</th>\n",
       "      <th>total</th>\n",
       "      <th>AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>616</td>\n",
       "      <td>207</td>\n",
       "      <td>18484</td>\n",
       "      <td>4693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1349</td>\n",
       "      <td>546</td>\n",
       "      <td>18145</td>\n",
       "      <td>3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_xgb</td>\n",
       "      <td>24000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4223</td>\n",
       "      <td>10</td>\n",
       "      <td>18681</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>predicted_prob_nn</td>\n",
       "      <td>24000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1635</td>\n",
       "      <td>657</td>\n",
       "      <td>18034</td>\n",
       "      <td>3674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_lr</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>148</td>\n",
       "      <td>59</td>\n",
       "      <td>4614</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_rf</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>325</td>\n",
       "      <td>146</td>\n",
       "      <td>4527</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_xgb</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.81</td>\n",
       "      <td>404</td>\n",
       "      <td>204</td>\n",
       "      <td>4469</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>predicted_prob_nn</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>389</td>\n",
       "      <td>178</td>\n",
       "      <td>4495</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type           predictor  total   AUC  accuracy    TP   FP     TN    FN\n",
       "0  training   predicted_prob_lr  24000  0.73      0.80   616  207  18484  4693\n",
       "1  training   predicted_prob_rf  24000  0.77      0.81  1349  546  18145  3960\n",
       "2  training  predicted_prob_xgb  24000  1.00      0.95  4223   10  18681  1086\n",
       "3  training   predicted_prob_nn  24000  0.78      0.82  1635  657  18034  3674\n",
       "4      test   predicted_prob_lr   6000  0.73      0.79   148   59   4614  1179\n",
       "5      test   predicted_prob_rf   6000  0.77      0.81   325  146   4527  1002\n",
       "6      test  predicted_prob_xgb   6000  0.75      0.81   404  204   4469   923\n",
       "7      test   predicted_prob_nn   6000  0.77      0.81   389  178   4495   938"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsm.evalbin(dct, rvar = 'DEFAULT', lev = 1, pred = ['predicted_prob_lr', 'predicted_prob_rf', 'predicted_prob_xgb', 'predicted_prob_nn'], cost = 3, margin=5, dec=2)[['Type', 'predictor', 'total', 'AUC', 'accuracy', 'TP', 'FP', 'TN', 'FN']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5900d54",
   "metadata": {},
   "source": [
    "The random forest, XGB and neural network have similar performance. The value of detecting a true defaulter balanced by cost of falsely flagging a defaulter can be assigned to arrive at a profit calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2d6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
